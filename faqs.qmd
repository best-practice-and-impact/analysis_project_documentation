---
title: "Frequently Asked Questions"
format:
  html:
    embed-resources: true
---

::: {.callout-note collapse="false"}
This guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.

Please get in touch with feedback to support the guidance by creating a [GitHub Issue](https://github.com/best-practice-and-impact/analysis_project_documentation/issues) or [emailing us](mailto:ASAP@ons.gov.uk).
:::

## When should I use this guidance?

If you are starting work on a new analysis project we recommend that you use this guidance right from the scoping stage. For teams who are in the middle of an analysis, you can still use the guidance and templates to think about the quality questions from the scoping and design stages and continue recording the answers up until the delivery stage. 

## Which analysis is in scope of the guidance?

The guidance applies to **all** analysis workflows, including: 

* analysis to support research, 
* advice for other departments, 
* ad-hoc and one-off outputs 
* and regular outputs.  

::: {.callout-note collapse="true"}

## Regular outputs as business critical models

Regular outputs often fulfil at least one of the criteria for what makes a business critical model:    
 
* Underpins essential financial and funding decisions;    
* Essential to the achievement of business plan actions and priorities;    
* Must be error free or else risk serious financial or legal penalties or reputational damage for the organisation.    
    
Business critical analysis requires the highest level of scrutiny and the governance arrangements must be appropriate for the level of risk.  
:::

::: {.callout-note collapse="true"}

## How should the questions be used for ad-hoc analysis?

For ad-hoc outputs with quick turnarounds and limited time and resources, analysis still needs sufficient quality assurance to ensure it is fit for purpose. Ad-hoc outputs are often used as part of the evidence when making important policy decisions, so getting them right is very important.  

We strongly encourage approvers of both regular and ad-hoc analysis to answer all questions relevant to their role. 
:::

::: {.callout-note collapse="true"}

## Why should these questions and assurance logs be completed?
Completing the logs makes it much easier to understand the risks that the analysis carries and will help you plan to mitigate them. Having a proper audit trail and comprehensive logs also help when:

* You need to explain why the analysis works as it does.    
* You need to raise quality and resourcing issues and push back against demands that are unrealistic.    
* You need to understand and communicate potential risks.    
:::

::: {.callout-note collapse="true"}

## How should I identify the Approver, Commissioner, Analyst and Assurer?

The [AQuA Book](https://www.gov.uk/government/publications/the-aqua-book-guidance-on-producing-quality-analysis-for-government) sets out four roles responsible for the assurance of analysis:

* Commissioner       
* Analyst (who usually report to the approver)    
* Assurer    
* Approver (usually leads the analyst team)      

What matters here is that each assurance role is covered in your workflow and that individuals know about and accept their responsibilities, not the name of the role.    

As a project team, you should make sure that each role is in place and you understand how it will operate for you so that you have the right assurance. 

The AQuA Book makes no expectations about levels of seniority or grade of each of the occupiers of the roles. It does not specify whether roles should be held by a person or could be held by a committee or other governance group (such as a senior leadership team). 

The key consideration is whether or not the person or group that undertakes the assurance role have the skills and resources they need to meet the requirements of the role. How the roles are covered in an analysis workflow varies from project to project, depending on how the work is planned, managed and assured. 

We suggest that if roles are taken by individuals, the SRO and commissioner should usually be at Grade 7 or above. If you are still unsure about how to allocate the roles among your team and governance groups, please [email us](mailto:ASAP@ons.gov.uk) with "analysis assurance" in the subject header and we will help you to make the decision. 
:::

::: {.callout-note collapse="true"}

## What help is available to answer the quality questions?

For ONS staff, the [ONS Quality Central wiki](https://officenationalstatistics.sharepoint.com/sites/onswiki/SitePages/Quality-Central.aspx) contains lots of useful guidance, templates and mandatory training to help you work through and answer the quality questions. It includes the [ONS Quality Standard for Analysis](https://officenationalstatistics.sharepoint.com/sites/onswiki/SitePages/Quality_Standards.aspx).    

These cross-government resources are also likely to be useful:    
* [Government Data Quality Hub](https://www.gov.uk/government/organisations/government-data-quality-hub) [Quality Questions and Red Flags](https://dataqualityhub.github.io/resources-for-quality-analysis-external/) and [Government Data Quality Framework](https://www.gov.uk/government/publications/the-government-data-quality-framework)    
* [Office for Statistics Regulation](https://osr.statisticsauthority.gov.uk/) [Quality Assurance of Administrative Data toolkit](https://osr.statisticsauthority.gov.uk/publication/administrative-data-quality-assurance-toolkit/) and [Quality and statistics: an OSR perspective](https://osr.statisticsauthority.gov.uk/publication/quality-and-statistics-an-osr-perspective/).    
* [The Uncertainty Toolkit for Analysts in Government](https://analystsuncertaintytoolkit.github.io/UncertaintyWeb/index.html)
::: 

::: {.callout-note collapse="true"}

## What are Black Box Models in AI, and why do they matter for quality assurance?

Black box models are AI systems whose internal workings are not easily understood or explained, even by their developers. These models—often based on complex algorithms like deep learning—can produce highly accurate results, but their decision-making processes are opaque.

Why is this a quality concern?
* Lack of transparency makes it difficult to verify how decisions are made.
* Hard to audit for fairness, bias, or compliance with regulations.
* Reduced trust from users and stakeholders who need clarity and accountability.

How does AI assurance help?

AI assurance provides tools and processes to evaluate and communicate the trustworthiness of black box models. This includes:

* Explainability techniques to make outputs more understandable.
* Audits and impact assessments to check for ethical and legal risks.
* Testing and validation to ensure consistent and fair performance.

What should I look for when reviewing black box models?

* Is there a clear explanation of how the model works or why it made a decision?
* Has the model been tested for bias, fairness, and robustness?
* Are there documented assurance activities (e.g., audits, assessments)?
* Is the model aligned with principles like transparency, accountability, and safety?

You can find more guidance on AI assurance here. (https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance)
:::