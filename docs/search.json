[
  {
    "objectID": "sample_assumptions_table.html",
    "href": "sample_assumptions_table.html",
    "title": "Example assumptions log",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nThis log contains a list of assumptions used in an analysis of data from UK universities. It also provides a score for each assumption.",
    "crumbs": [
      "Support and Feedback",
      "Example assumptions log"
    ]
  },
  {
    "objectID": "sample_assumptions_table.html#definitions",
    "href": "sample_assumptions_table.html#definitions",
    "title": "Example assumptions log",
    "section": "Definitions",
    "text": "Definitions\nAssumptions are scored as Red, Amber or Green (a RAG rating) depending on quality. In RAG, green denotes a favourable value, red unfavourable and amber neutral. The quality of an assumption measures both how certain and robust an assumption is and how appropriate it is for its intended use.\nFor example, we would usually consider a well documented assumption drawn from published evidence to be very robust, but if it needs to be transformed or adapted significantly to fit the analysis, the quality rating might need downgrading.\nYou would normally lower the quality rating of an assumption if you cannot get technical sign-off (for example because of lack of technical knowledge) or if the information on which it is based is incomplete or poor quality. You would also normally lower the quality if the confidence interval or uncertainty range is wide (i.e. you wouldn’t be surprised if the value was 50% different from what you measure because of uncertainty).\n\n\n\n\n\n\n\nRAG Rating\nAssumption quality\n\n\n\n\nGREEN\nBased on validated data; Methodology is robust; No or few transformations, or transformation methodology is fully verified and robust; Data is current and signed off by experts; Confidence intervals are narrow.\n\n\nAMBER\nThe methodology is robust but based on limited data; Data required significant transformation to fit the model; Confidence interval is quite wide; Data has not been reviewed recently.\n\n\nRED\nUnclear/unreliable data source or no data source provided; Based on limited data and methodology not robust; Data is not current; Confidence interval is wide or quality is unknown.\n\n\n\n\n\n\n\nAssumption ID\n\n\nDepends on Assumptions\n\n\nLocation in code, documentation or publication\n\n\nPlain English description of assumption\n\n\nBasis for assumption\n\n\nNumerical value of the assumption\n\n\nRange around the estimated value\n\n\nEstimated distribution\n\n\nLinks to supporting analysis\n\n\nDocumentation dependencies\n\n\nDate of last review/update\n\n\nExternally reviewed by\n\n\nDate of external review\n\n\nNext review/update due on\n\n\nQuality rating\n\n\nSensitivity score\n\n\nRisk score\n\n\n\n\n1\n\n\n2,3,4,5,6,7,8,9,10\n\n\nAssumption log\n\n\nWe assume that the dataset is representative of the population.\n\n\nTeam opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nDescriptive statistics (link), comparison to existing data source/publication (link)\n\n\nFinal report: methods, caveats\n\n\n14/02/2024\n\n\nJohn Doe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nHigh\n\n\nHigh\n\n\n\n\n2\n\n\n3,4,5,6,8,9,10\n\n\nAssumption log\n\n\nWe assume that the data does not exclude any population groups based on their demographic and socio-economic characteristics.\n\n\nTeam opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nDescriptive statistics (link), comparison to existing data source/publication (link)\n\n\nFinal report: methods, descriptive statistics, caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nHigh\n\n\nHigh\n\n\n\n\n3\n\n\n\n\nCorrespondence with data provider\n\n\nWe assume that all UK universities report data to the Higher Education Statistics Agency (HESA).\n\n\nValidated from data provider, coverage check against university list\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nHigh\n\n\nMedium\n\n\n\n\n4\n\n\n3\n\n\nCorrespondence with data provider\n\n\nWe assume that our list of UK universities is correct, current and comprehensive.\n\n\nTeam opinion, reliable source (HESA list)\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nLow\n\n\nMedium\n\n\n\n\n5\n\n\n2,3,4\n\n\nAssumption log\n\n\nWe assume that all universities accurately report the number of students enrolled during the academic year.\n\n\nExpert opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nFinal report: Caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nAMBER\n\n\nHigh\n\n\nHigh\n\n\n\n\n6\n\n\n3,4\n\n\nAssumption log\n\n\nWe assume that all universities accurately report the number of students who dropped out during the academic year.\n\n\nExpert opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nPublication on drop-out rates\n\n\nFinal report: Caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n12/05/2024\n\n\nAMBER\n\n\nHigh\n\n\nHigh\n\n\n\n\n7\n\n\n3\n\n\nCorrespondence with data providers.\n\n\nWe assume that the academic year is consistently measured across UK universities.\n\n\nValidated from data provider\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nQuality assurance based on sampling universities from their websites\n\n\nQuality assurance log, final report: caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nRED\n\n\nHigh\n\n\nHigh\n\n\n\n\n8\n\n\n9\n\n\nCorrespondence with data providers.\n\n\nWe assume that students who receive special education services are excluded from the calculation of dropout rates.\n\n\nValidated from data provider\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nSensitivity analysis comparing dropout rates with and without this population included.\n\n\nQuality assurance log, final report: methods, caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nRED\n\n\nHigh\n\n\nHigh\n\n\n\n\n9\n\n\n2,3,4,10\n\n\nExploratory data analysis notebook (link)\n\n\nWe assume that there is complete information for all the variables in the analysis.\n\n\nRobustness testing\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nDescriptive statistics notebook link\n\n\nDesk instructions, final report: methods, summarising the sample\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nAMBER\n\n\nHigh\n\n\nHigh\n\n\n\n\n10\n\n\n2,3,4,9\n\n\nCorrespondence with data provider\n\n\nWe assume that the data collection process has not changed at all over time.\n\n\nValidated from data provider (link)\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nMethods documents from prior runs of this work, supplier data specifications and quality reports\n\n\nData supply specification, data quality report, methods document\n\n\nNot yet assigned\n\n\nNo external review\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nRED\n\n\nHigh\n\n\nHigh\n\n\n\n\n11\n\n\n2,3,4,5,6,7,8\n\n\nAssumption log\n\n\nWe assume that the correlation coefficient between the dropout rate and social grade of local authority of origin is 0.7\n\n\nStatistical analysis of past data\n\n\n0.7\n\n\n+-0.1\n\n\nNormal\n\n\nCorrelation analysis report m(link), comparison to domain knowledge (link)\n\n\nData analysis documentation\n\n\n05/12/2023\n\n\nJohn Doe\n\n\n05/12/2023\n\n\n01/05/2024\n\n\nGREEN\n\n\nMedium\n\n\nLow",
    "crumbs": [
      "Support and Feedback",
      "Example assumptions log"
    ]
  },
  {
    "objectID": "quality_questions.html",
    "href": "quality_questions.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nTo get the most out of the template, we strongly recommend that teams identify who will take the key quality assurance roles of Commissioner, Approver and Assurer and name the analytical team at the start of the analytical cycle. This is crucial because together these roles help to make sure that the analysis you do is fit-for-purpose.",
    "crumbs": [
      "ONS QUALITY QUESTIONS"
    ]
  },
  {
    "objectID": "quality_questions.html#iv.-delivery",
    "href": "quality_questions.html#iv.-delivery",
    "title": "Quality Questions",
    "section": "IV. Delivery",
    "text": "IV. Delivery\n\nQuality questions and why they matterThe questions and the Code of PracticeLinking the questions to AQuA roles\n\n\n\n\n\n\n\nQuality Question\n\n\nWhy do I need to know the answer to this?\n\n\n\n\nQ53\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nOften the aim of final output is to inform decison-making. Output might include predictions, involving lots of underlying assumptions. It is critical that you support your users to make appropriate use of outputs and understand what can and cannot be inferred. Without this, users may misinterpret findings, make in appropriate comparisons, use the analysis for unsuitable purposes and arrive at the wrong conclusions. For example, a non-expert user may wrongly interpret correlation as causation or use incomplete or disconnected data to make forecasts.\n\n\n\n\nQ54\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nYou should describe why limitations related to data and methods exist, why they cannot be overcome using the chosen approach and their impact on the quality and interpretation of the output. Analysis is of very little value if limitations aren’t properly documented and explained.\n\n\n\n\nQ55\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nYou should work with users, experts, and other relevant stakeholders to verify the credibility of outputs and sense check that they are useful.\n\n\n\n\nQ56\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nOutputs are never 100% accurate. Users need to understand how uncertainties related to data, assumptions and methodology feed into and through the analysis workflow and what this means for the use of the outputs. Results must clearly explain how uncertainty affects the findings from the analysis, or we risk misinterpretations and conclusions being overly reliant on imprecise results.\n\n\n\n\nQ57\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nYou must support your users to understand relevant uncertainties which are not captured in the analysis. When you can, make reasonable judgements about the likely size and direction of unquantified uncertainty. Provide a qualitative description informing users about why the uncertainty cannot be quantified and their likely impact.\n\n\n\n\nQ58\n\n\nIs workflow documentation including technical guides and code repositories publicly available?\n\n\nTransparency about your analysis supports proper scrutiny and challenge, promotes public trust, and encourages re-use of the resources you develop.\n\n\n\n\nQ59\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nA good technical guide helps everybody to understand what the analysis does and how it works. A well-written technical guide is essential for effective maintenance of the analysis. It helps users of the analysis to replicate the findings, get answers to methodology questions and build their trust in the output.\n\n\n\n\nQ60\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nThe technical guide is complemented by fully documented analysis code. Code documentation must comply with good practice so new users can understand and execute the code as easily and quickly as possible.\n\n\n\n\nQ61\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nExternal critique makes analysis more robust. Users should be able to give feedback to your team to ensure that results meet their needs. User feedback and customer reviews inform you of issues and changes that you might need to make. They also act as evidence that users have been consulted.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich pillar and principle of Code of Practice matter here?\n*Trustworthiness (T), Quality (Q), Value (V)\n\n\n\n\nQ53\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ54\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.Q1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.Q3.1 Statistics should be produced to a level of quality that meets users’ needs. The strengths and limitations of the statistics and data should be considered in relation to different uses, and clearly explained alongside the statistics.\n\n\n\n\nQ55\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.\n\n\n\n\nQ56\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nQ1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.Q2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ57\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.Q3.3 The extent and nature of any uncertainty in the estimates should be clearly explained.\n\n\n\n\nQ58\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.\n\n\n\n\nQ59\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.V3 Statistics and data should be presented clearly, explained meaningfully and provide authoritative insights that serve the public good.\n\n\n\n\nQ60\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nT4.4 Good business practices should be maintained in the use of resources. Where appropriate, statistics producers should take opportunities to share resources and collaborate to achieve common goals and produce coherent statistics.Q2.1 Methods and processes should be based on national or international good practice, scientific principles, or established professional consensus.\n\n\n\n\nQ61\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nV1 Users of statistics and data should be at the centre of statistical production; their needs should be understood, their views sought and acted upon, and their use of statistics supported. V1.4 Statistics producers should engage publicly through a variety of means that are appropriate to the needs of different audiences and proportionate to the potential of the statistics to serve the public good. An open dialogue should be maintained using proactive formal and informal engagement to listen to the views of new and established contacts. Statistics producers should undertake public engagement collaboratively wherever possible, working in partnership with policy makers and other statistics producers to obtain the views of stakeholders.V1.5 The views received from users, potential users and other stakeholders should be addressed, where practicable. Statistics producers should consider whether to produce new statistics to meet identified information gaps. Feedback should be provided to them about how their needs can and cannot be met, being transparent about reasons for the decisions made and any constraints.\n\n\n\n\n\n\n\n\n\n\n\nQuality Question\n\n\nWhich AQuA role(s) would normally answer this?\n\n\nWhy are these AQuA roles involved?\n\n\n\n\nQ53\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nCommissioner, analyst\n\n\nDuring the delivery phase, the commissioner receives the results of the analysis and decides whether it meets their needs. The analyst provides sufficient information to support the commissioner to make an informed decision.\n\n\n\n\nQ54\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nCommissioner, Assurer, Analyst\n\n\nThe commissioner must be confident in the quality of the outputs. They should understand the strengths, limitations and context of the analysis so that the results are correctly interpreted. Assurer sign-off provides confidence that analysis risks, limitations and major assumptions are understood by the users of the analysis. Analysts make sure that the commissioner and assurer have the evidence they need.\n\n\n\n\nQ55\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nAnalyst, Assurer\n\n\nThe analyst and assurer should enable and encourage peer review. Peer reviews provide useful critical challenge about the analytical approach, application of methods and interpretation of the analysis. Verification and peer review of work should be done by analysts who had no involvemen in the work  so their views are independent.\n\n\n\n\nQ56\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nAnalyst, commissioner\n\n\nThe analyst must determine and communicate the uncertainty associated with the analysis so the commissioner can make informed decisions. The commissioner should ensure that an assessment of uncertainty has been provided and that the implications of uncertainty are understood.\n\n\n\n\nQ57\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nAnalyst, commissioner\n\n\nIf uncertainty is too complex to quantify, even approximately, the analysts should explain this so the commissioner can take this into account. In communicating analysis results to decision-makers and stakeholders, the commissioner should be open about the existence of deep uncertainties whose impact cannot be assessed, and explain how they are managed in the analysis.\n\n\n\n\nQ58\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nAnalyst, Assurer\n\n\nThe analyst must produce appropriate design documentation. Best practice includes maintaining a record of the analysis workflow in a technical report, including a concept of analysis, user requirements, design specification, functional specification, data dictionary, and test plan. Code should be properly documented.\n\n\n\n\nQ59\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nAnalyst, Assurer\n\n\nThe analyst must produce appropriate documentation. Best practice includes maintaining a record of the work that has been done in a technical report, including a full description of the analysis, user requirements, design specification, functional specification, data dictionary, and test plan. The assurer makes sure that the documentation is fit for purpose.\n\n\n\n\nQ60\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nAnalyst, Assurer\n\n\nAnalysts should develop and maintain analysis code in line with best practice. Code must comply with relevant policies and standards.\n\n\n\n\nQ61\n\n\nIs there a clear feedback mechanism so users can report back on the suitability of outputs?\n\n\nAnalyst, Approver\n\n\nYou can assess the usefulness of the analysis by getting feedback from users, stakeholders and other experts. Quality analysis should be free of prejudice or bias. The SRO and analysts should check that the analysis follows the principles of RIGOUR (Repeatable, Independent, Grounded in reality, Objective, Uncertainty-managed, Robust)",
    "crumbs": [
      "ONS QUALITY QUESTIONS"
    ]
  },
  {
    "objectID": "how_to_use.html",
    "href": "how_to_use.html",
    "title": "How to use ONS Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "How to use the ONS Quality Questions"
    ]
  },
  {
    "objectID": "how_to_use.html#structure-of-the-ons-quality-questions",
    "href": "how_to_use.html#structure-of-the-ons-quality-questions",
    "title": "How to use ONS Quality Questions",
    "section": "Structure of the ONS Quality Questions",
    "text": "Structure of the ONS Quality Questions\nThere are 61 quality questions in total. They cover all the stages of the analytical cycle.\nAnswering 61 questions must seem quite daunting at first! The questions are designed to help you as you work your way through the analytical process, rather than to be answered all at once. Creating a log of answers as you move through the stages of the analysis workflow will help you to check that your work meets analytical standards, follows good practice and (if relevant) complies with the Code of Practice for Statistics.\nAnswering the questions will also help you make sure that everybody working on the analysis has a clear understanding of how and why it works as it does, and to support your users when writing your outputs. Moreover, most of the answers will help you to produce the critical documents that mitigate risk like an assumptions log, decisions log, issues log, and technical guides for your team and your users.\nThe AQuA book divides the analytical cycle into four stages:\n\nScoping\n\nDesign\n\nConducting and checking analysis\n\nDelivery\n\nYou can find the questions in the ONS Quality Questions section of the website. The questions have tabs which relate to three themes:\n\n\nTheme1: ONS Quality Questions and why they matter\n\nAlongside each question we explain why it matters and explain the potential risks and benefits around it. We try to address the very sensible question of “why should I care about this?”.\n\n\n\nTheme 2: The questions and the Code of Practice for Statistics\n\nEach question is linked with the Code of Practice for Statistics pillars of Trustworthiness, Quality and Value. We explain the importance and relevance of each question in light of the three pillars so teams can better understand and apply these principles through out the project life cycle.\nEven if your work does not directly feed into the production of official statistics, compliance with the principles and practices of the Code is a good way to strengthen the resilience of your work, increase transparency and clarity and reduce risk.\n\n\n\nTheme 3: Linking the questions to AQuA roles\n\nQuality questions are also linked to which responsible role from the AQuA book would usually answer them. The idea is to highlight the clear line of accountability set out in the AQuA book in an easy-to-understand manner. We want to make it easier for teams to decide how the four key assurance roles of Commissioner, Approver, Analyst and Assurer are covered in their own workflows.\nThe AQuA book sets out four roles that cover different areas of assurance responsibility. Taken together, they provide a comprehensive set of assurance for an analytical project. The roles are:\n\nCommissioner (may be known as customer)\n\nAnalyst\n\nAssurer (may be known as the analytical assurer or assuring analyst)\nApprover (may be known as senior analyst or senior responsible officer)\n\nLet’s look at the roles more detail.\nResponsibilities of the Commissioner\nThe commissioner is focused on making sure the analysis meets the required user needs.\nEnsures that the context around the work is understood, so quality assurance is appropriate and proportionate. Ensures that there is enough time and resource for required assurance, and accounts for risk. Must understand strengths and limitations including uncertainty, so results are interpreted correctly. Delivers QUALITY OF OUTCOME — “The analysis meets user needs and we understand its limitations.” Requests the analysis and sets out their requirements. Agrees that what the analyst is going to do will satisfy the need. Accepts the analysis and assurance as fit for purpose.\nResponsibilities of the Approver\nThe Approver is accountable for the analytical workflow throughout its lifecycle. Usually a senior member of the analytical team, they work closely with (or manage) the analyst team.\nSigns off all important decisions made about the analysis to ensure that it is fit-for-purpose prior to use. Scrutinises the work of the analyst and the assurer. Confirms (if necessary) to the analyst, assurer, and commissioner that the work has been appropriately assured. There is no minimum grade for the SRO role, but they should have the expertise, resources and accountability to ensure that the analysis is well-designed, complies with relevant standards, works as intended and is fit for purpose. Delivers QUALITY OF CONTENT — “The analysis is well-designed and uses the right tools and methods to meet user needs.” — alongside the analyst team.\nResponsibilities of the Analyst\nThe analyst team usually works to the SRO. Analysts are responsible for setting up, running, checking and reporting on the analysis.\nAssist the Commissioner and SRO in framing the question to ensure the right analysis is done. Manage external specialists. Design, build, document, and run analyses. Carries out their own assurance. Acts on findings from the assurer. Can be a group of analysts, in which case the lead analyst is responsible. Delivers QUALITY OF CONTENT — “The analysis is well-designed and uses the right tools and methods to meet user needs.” — alongside the SRO.\nResponsibilities of the Assurer\nThe assurer is there to make sure high-level assurance takes place.\nAssurance takes place throughout the lifecycle — from design through to use. Reviews the assurance completed by the analyst. Carries out any further validation and verification they may see as appropriate. Reports errors and areas for improvement to the analyst. Undertakes repeated reviews as required. Confirms to the approver that the work has been appropriately scoped, executed, validated, verified, and documented. Must be independent from the analyst. Can be a group of assurers, in which case the leader of the group is responsible. Delivers QUALITY OF PROCESS — “The analysis does what it’s supposed to do — and we can prove it.”\nYou can read more about the four roles in the AQuA Book and Verification and Validation for the AQuA Book.",
    "crumbs": [
      "How to use the ONS Quality Questions"
    ]
  },
  {
    "objectID": "how_to_use.html#answering-the-ons-quality-questions",
    "href": "how_to_use.html#answering-the-ons-quality-questions",
    "title": "How to use ONS Quality Questions",
    "section": "Answering the ONS Quality Questions",
    "text": "Answering the ONS Quality Questions\nWe have made templates for you to download and use to record your answers to the questions, as well as key project information. There is an Excel template and an editable HTML template.\nSave the template in your project repository or on Sharepoint so that changes can be tracked through version control. You can update it as your analytical work progresses.\nWe encourage the commissioner, analyst team and assurer to contribute response to all the questions that are relevant to their responsibilities. To keep the level of assurance proportionate to your analysis it might be decided that only a subset of questions need be covered.\n\n\n\n\n\n\nNoteIs there any help in deciding which questions should be covered?\n\n\n\n\n\nThe commissioner and lead analyst should agree the level of assurance required and identify which questions are most appropriate together with any possible additional ones.\nThe Excel template has mapped the questions to various categories including Analysis Function’s Quality Questions and Red Flags and the OSR’s Statistical Thinking – both these publications promote their questions as a core set that anyone doing analysis should consider. To identify these questions it is only necessary to filter on the appropriate columns in the worksheet.\nIt is also possible to filter questions relevant for each stage of the Generic Statistical Business Process Model. Many questions are relevant for more than GSBPM stage and only need be answered once!\nWe aim to add further categories to subset questions best suitable for production, research and adhoc projects. These would only be suggestive but could provide a start.\n\n\n\nFor audit purposes, remember to sign and date your responses.\nWhen analysis is due to complete the lead analyst or their designated representative should provide sign-off for the whole template if satisfied that responses provide sufficient assurance of the analysis.",
    "crumbs": [
      "How to use the ONS Quality Questions"
    ]
  },
  {
    "objectID": "faqs.html",
    "href": "faqs.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "Support and Feedback",
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#when-should-i-use-this-guidance",
    "href": "faqs.html#when-should-i-use-this-guidance",
    "title": "Frequently Asked Questions",
    "section": "When should I use this guidance?",
    "text": "When should I use this guidance?\nIf you are starting work on a new analysis project we recommend that you use this guidance right from the scoping stage. For teams who are in the middle of an analysis, you can still use the guidance and templates to think about the quality questions from the scoping and design stages and continue recording the answers up until the delivery stage.",
    "crumbs": [
      "Support and Feedback",
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "href": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "title": "Frequently Asked Questions",
    "section": "Which analysis is in scope of the guidance?",
    "text": "Which analysis is in scope of the guidance?\nThe guidance applies to all analysis workflows, including:\n\nanalysis to support research,\nadvice for other departments,\nad-hoc and one-off outputs\nand regular outputs.\n\n\n\n\n\n\n\nNoteRegular outputs as business critical models\n\n\n\n\n\nRegular outputs often fulfil at least one of the criteria for what makes a business critical model:\n\nUnderpins essential financial and funding decisions;\n\nEssential to the achievement of business plan actions and priorities;\n\nMust be error free or else risk serious financial or legal penalties or reputational damage for the organisation.\n\nBusiness critical analysis requires the highest level of scrutiny and the governance arrangements must be appropriate for the level of risk.\n\n\n\n\n\n\n\n\n\nNoteHow should the questions be used for ad-hoc analysis?\n\n\n\n\n\nFor ad-hoc outputs with quick turnarounds and limited time and resources, analysis still needs sufficient quality assurance to ensure it is fit for purpose. Ad-hoc outputs are often used as part of the evidence when making important policy decisions, so getting them right is very important.\nWe strongly encourage approvers of both regular and ad-hoc analysis to answer all questions relevant to their role.\n\n\n\n\n\n\n\n\n\nNoteWhy should these questions and assurance logs be completed?\n\n\n\n\n\nCompleting the logs makes it much easier to understand the risks that the analysis carries and will help you plan to mitigate them. Having a proper audit trail and comprehensive logs also help when:\n\nYou need to explain why the analysis works as it does.\n\nYou need to raise quality and resourcing issues and push back against demands that are unrealistic.\n\nYou need to understand and communicate potential risks.\n\n\n\n\n\n\n\n\n\n\n\nNoteHow should I identify the Approver, Commissioner, Analyst and Assurer?\n\n\n\n\n\nThe AQuA Book sets out four roles responsible for the assurance of analysis:\n\nCommissioner\n\nAnalyst (who usually report to the approver)\n\nAssurer\n\nApprover (usually leads the analyst team)\n\nWhat matters here is that each assurance role is covered in your workflow and that individuals know about and accept their responsibilities, not the name of the role.\nAs a project team, you should make sure that each role is in place and you understand how it will operate for you so that you have the right assurance.\nThe AQuA Book makes no expectations about levels of seniority or grade of each of the occupiers of the roles. It does not specify whether roles should be held by a person or could be held by a committee or other governance group (such as a senior leadership team).\nThe key consideration is whether or not the person or group that undertakes the assurance role have the skills and resources they need to meet the requirements of the role. How the roles are covered in an analysis workflow varies from project to project, depending on how the work is planned, managed and assured.\nWe suggest that if roles are taken by individuals, the SRO and commissioner should usually be at Grade 7 or above. If you are still unsure about how to allocate the roles among your team and governance groups, please email us with “analysis assurance” in the subject header and we will help you to make the decision.\n\n\n\n\n\n\n\n\n\nNoteWhat help is available to answer the quality questions?\n\n\n\n\n\nFor ONS staff, the ONS Quality Central wiki contains lots of useful guidance, templates and mandatory training to help you work through and answer the quality questions. It includes the ONS Quality Standard for Analysis.\nThese cross-government resources are also likely to be useful:\n* Government Data Quality Hub Quality Questions and Red Flags and Government Data Quality Framework\n* Office for Statistics Regulation Quality Assurance of Administrative Data toolkit and Quality and statistics: an OSR perspective.\n* The Uncertainty Toolkit for Analysts in Government\n\n\n\n\n\n\n\n\n\nNoteWhat are Black Box Models in AI, and why do they matter for quality assurance?\n\n\n\n\n\nBlack box models are AI systems whose internal workings are not easily understood or explained, even by their developers. These models—often based on complex algorithms like deep learning—can produce highly accurate results, but their decision-making processes are opaque.\nWhy is this a quality concern? * Lack of transparency makes it difficult to verify how decisions are made. * Hard to audit for fairness, bias, or compliance with regulations. * Reduced trust from users and stakeholders who need clarity and accountability.\nHow does AI assurance help?\nAI assurance provides tools and processes to evaluate and communicate the trustworthiness of black box models. This includes:\n\nExplainability techniques to make outputs more understandable.\nAudits and impact assessments to check for ethical and legal risks.\nTesting and validation to ensure consistent and fair performance.\n\nWhat should I look for when reviewing black box models?\n\nIs there a clear explanation of how the model works or why it made a decision?\nHas the model been tested for bias, fairness, and robustness?\nAre there documented assurance activities (e.g., audits, assessments)?\nIs the model aligned with principles like transparency, accountability, and safety?\n\nYou can find more guidance on AI assurance here. (https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance)",
    "crumbs": [
      "Support and Feedback",
      "FAQs"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html",
    "href": "assumptions_and_issues_log.html",
    "title": "Assumptions and decision log guide",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nAnalysis always involves assumptions and decisions. It always has limitations, which might mean that it is not fit for purpose for every use case.\nKeeping a formal record of assumptions, decisions and key limitations (such as the uncertainty involved in the analysis) and bringing them together in one place means that everybody involved in the work can find them quickly, see what they are and understand how they might affect the overall analysis.\nIt also makes it much easier to:\nWe have developed an assumptions and decisions log template for you to download that provides the basis for recording your assumptions, decisions and limitations (identified as risks and issues). This template includes multiple worksheets to record fuller project information giving a more comprehensive understanding of your project\nDetails for recording information in each log is as follows.",
    "crumbs": [
      "Assumptions and decision log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#field-definitions",
    "href": "assumptions_and_issues_log.html#field-definitions",
    "title": "Assumptions and decision log guide",
    "section": "Field definitions",
    "text": "Field definitions\n\n\n\n\n\n\n\nField\nDefinition\n\n\n\n\nAssumption ID:\nGive each assumption a unique ID so that it can be tracked easily and cross referenced.\n\n\nThis Assumption depends on other Assumptions:\nIdentify the IDs of dependencies\n\n\nLocation in code, documentation or publication:\nWrite the name of the document or publication where the assumption is applied. If the assumption is applied in your code, identify where the assumption applies by citing the module and line number.\n\n\nPlain English description of assumption:\nWrite a clear description of the assumption in plain English. For example, “We assume that our sample of income data is representative for the whole population”.\n\n\nBasis for assumption:\nBriefly summarise why the assumption matters for the analysis and why it is reasonable. For example, the assumption could be based on historical data, theoretical requirements of the method, empirical evidence, quality assurance, common practice or data testing.\n\n\nNumerical value of the assumption:\nIf you can, attach a numerical value to the assumption. This will depend on what the assumption is and how it is applied in your work. For example, you might make an assumption that counts in your data are accurate to within 5 units, so you would record +/- 5 as the numerical value. Thinking about the value of the assumption in numeric terms is a useful way to work through its impact on your work.\n\n\nRange around the estimated value:\nAssumptions are rarely certain. If you can, assign a range to the central value of the assumption.\n\n\nLinks to supporting analysis:\nAttach a link to the source where the assumption comes from or from where it can be verified and justified.\n\n\nDocumentation dependencies:\nList the project documents dealing with this assumption. Assumptions can have an impact on different stages of the analysis. For example, an assumption about input data if valid or in-valid could impact the model code, project timeline, robustness of the outputs produced by the model. Logging these dependencies will ensure that the relevant plans and documents are updated once an assumption is validated.\n\n\nInternally reviewed by:\nEnter the full name of individual or group who reviewed the assumption.\n\n\nDate of last review/update:\nEnter the date the assumption was last reviewed or validated.\n\n\nExternally reviewed by:\nEnter the name of organisation and the position of the expert who validated the assumption. It is not necessary to mention the individual’s name. For example, Lighthouse Laboratory (Senior Scientist), University of Oxford (Professor of Statistics).\n\n\nDate of external review:\nEnter the date the assumption was reviewed by the external expert.\n\n\nNext review/update due on:\nEnter the date the assumption next needs to be reviewed or updated.\n\n\nQuality Rating:\nUse the Quality Rating key (on the Quality and Sensitivity Guide worksheet) to assess quality of assumption as red, amber or green.\n\n\nSensitivity Score:\nUse the Sensitivity Rating key (on the Quality and Sensitivity Guide worksheet) to assess sensitivity as low, medium or high.\n\n\nRisk Score:\nUse the Risk Scoring Guide worksheet to assess the assumption risk as low, medium or high.",
    "crumbs": [
      "Assumptions and decision log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#field-definitions-1",
    "href": "assumptions_and_issues_log.html#field-definitions-1",
    "title": "Assumptions and decision log guide",
    "section": "Field definitions",
    "text": "Field definitions\n\n\n\n\n\n\n\nField\nDefinition\n\n\n\n\nDecision ID:\nGive each decision a unique ID so that it can be tracked easily and cross referenced.\n\n\nDecision name:\nEnter a name for the decision.\n\n\nDate of decision:\nEnter the date the decision was made.\n\n\nPlain English description of decision\nA brief summary of the decision in plain English explaining what it was about and how it applies in the analysis.\n\n\nName of person or group signing off the decision:\nEnter the name of the person or group who made the decision. If the decision was made by a committee, link to the minute where the decision is documented.\n\n\nRole of person or group signing off the decision:\nProvide a short description of the person or group’s role in the project. Decisions are often signed off by the project lead or Senior Responsible Owner, for example.\n\n\nDate of last review / update:\nDate the decision was last considered. This is here so you know when the decision was last looked at. Projects evolve, and decisions might need to be reviewed.\n\n\nReviewed by:\nName(s) of people or group who last reviewed the decision.\n\n\nNext review/update due on:\nEnter the date the decision next needs to be reviewed or updated.",
    "crumbs": [
      "Assumptions and decision log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#field-definitions-2",
    "href": "assumptions_and_issues_log.html#field-definitions-2",
    "title": "Assumptions and decision log guide",
    "section": "Field definitions",
    "text": "Field definitions\n\n\n\n\n\n\n\nField\nDefinition\n\n\n\n\nIssue ID:\nGive each issue a unique ID so that it can be tracked easily and cross referenced.\n\n\nIssue name:\nEnter a name for the issue. It could be anything raised by the team, anything raised during quality assurance by the team members or external reviewers. Include the location of the issue, for example, line number of code, error in publication, where it arises in the workflow, resource constraint.\n\n\nDate identified:\nEnter the date the issue was first identified.\n\n\nPlain English description of issue:\nA brief summary of the underlying cause and nature of issue in plain English explaining what is creating problem.\n\n\nImpact of issue:\nBrief summary of how the issue affects the project, for example, timeline, accuracy, cost.\n\n\nStatus of issue:\nFrom the dropdown menu, select ‘Live’ if the issue is being handled, ‘Resolved’ if the issue has been resolved or ‘Untreated’ if the issue has yet to be handled.\n\n\nJustification of status:\nBrief summary of how the solution has resolved the issue if the status is ‘Resolved’. For ‘Untreated’ issues, explain why the issue will be dealt with later.\n\n\nProof of resolution:\nWrite the name of the output report/methodology paper or the published document where the issue is resolved. If the issue relates to code, identify the module and line number.\n\n\nDate of last review/update:\nEnter the date the issue was last reviewed or updated.\n\n\nReviewed by:\nEnter the full name of individual(s) or group who reviewed the issue.\n\n\nNext review/update due on:\nEnter the date the issue next needs to be reviewed or updated.",
    "crumbs": [
      "Assumptions and decision log guide"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Feedback",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\n\n\n\n\nFeedback\nWe are keen to understand the challenges faced by analytical teams in ONS when complying with good practice. To evaluate the effectiveness of this guidance, we would be grateful if you could fill in a short survey.\nWe appreciate your feedback. It will help us to make additional tools and resources on good practice for producing quality analysis.\nIf you have questions about this guidance or the survey please email ASAP@ons.gov.uk “analytical QA” in the subject header.",
    "crumbs": [
      "Support and Feedback",
      "Feedback"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ONS Quality Questions Introduction",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#who-is-this-guidance-for",
    "href": "index.html#who-is-this-guidance-for",
    "title": "ONS Quality Questions Introduction",
    "section": "Who is this guidance for?",
    "text": "Who is this guidance for?\nThis guidance provides a set of questions to help analytical and statistical teams evaluate the quality of their analysis throughout the production cycle.\nThe guidance is here to support teams in meeting the Office for National Statistics’s (ONS) strategic objectives for improving statistical quality. You can find more information about our strategic objectives on statistical quality in the ONS Statistical Quality Improvement Strategy. ONS manages quality through a strategic risk approach.\nWe have made the guidance available on Github in case others wish to use the Quality Questions resource in their own work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "ONS Quality Questions Introduction",
    "section": "Aims",
    "text": "Aims\nThe guidance has five main aims:\n\nTo help analysts understand the level of risk they are carrying in their analytical workflows.\nTo ensure there is a consistent end-to-end QA approach across ONS.\n\nTo make it easier to comply with good practice guidance and standards including the ONS Quality Practices, ONS Quality Standard for Analysis, the government AQUA Book and the Code of Practice for Statistics, the Analysis Function Functional Standard for Analysis and the Government Service Manual which explains how to research, document and validate user needs.\n\nTo ensure there is a consistent understanding of roles and responsibilities when producing high quality analysis and statistics.\n\nTo make it easier to create critical project documentation including an assumptions and decisions log, issue and decisions log, risk register and divisional Quality Improvement Plan.\n\nReflecting on the questions asked in the template will help you to manage your analysis risks:\n\nYou will be able to document the mitigation that is in place or planned.\n\nYou will know which issues and risks the project is prepared to accept and why.\n\nYou can identify potential quality issues and decide how to manage and prioritise them.\n\nHaving this information in once place provides a sound basis for regular reviews of assumptions, issues and risks associated with the workflow, in line with recommended good practice.\n\n\n\n\n\n\n\nNoteHow the questions draw on other frameworks\n\n\n\n\n\nThe AQuA book sets out a standard framework for managing analytical quality in HM Government. AQuA is there to make sure that our work can be trusted to inform good decision making, while the Code of Practice for Statistics sets out the principles and practices that producers of official statistics should commit to.\nTwo other pieces of guidance have motivated us to produce this template. One is the Analysis Function guidance on Quality Questions and Red Flags. The other is the Office for Statistics Regulation (OSR) guidance on Thinking about quality when producing statistics. Both of these provide sets of questions that analysts can use to interrogate their work and assure its quality.\nBuilding on these resources, this guidance sets out quality questions that are relevant for each stage of analytical cycle. The quality questions are at their most effective if they are asked at the right stage. Once that stage is passed, experience suggests that it is normally difficult to go back and address the points the questions ask by retrofitting at a later stage of the analysis.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "quality_questions_markdown_version.html",
    "href": "quality_questions_markdown_version.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nTo make the most out of this template, we strongly recommend that you identify who will take the three key quality assurance roles of Commissioner, Senior Responsible Owner and Analytical Assurer at the start of the analytical cycle. This is crucial as each of these individuals has a role in ensuring that the analysis you do is fit-for-purpose. You should also identify the members of the analytical team. If team members have specific roles, record them here and update them when they change."
  },
  {
    "objectID": "quality_questions_markdown_version.html#lead-analytical-roles",
    "href": "quality_questions_markdown_version.html#lead-analytical-roles",
    "title": "Quality Questions",
    "section": "Lead analytical roles",
    "text": "Lead analytical roles\n\n\n\n\n\n\n\n\n\nName of Senior Responsible Owner (SRO)\nName of Commissioner\nName of Analyst(s)\nName of Analytical Assurer(s)\n\n\n\n\nEnter name here\nEnter name here\nEnter name here\nEnter name here"
  },
  {
    "objectID": "quality_questions_markdown_version.html#quality-questions",
    "href": "quality_questions_markdown_version.html#quality-questions",
    "title": "Quality Questions",
    "section": "Quality Questions",
    "text": "Quality Questions\n\n\n\n\n\n\n\n\n\n\n\n\n\nScoping\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ1\nWhat question is the analysis trying to answer?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ2\nWhy do you need to answer this analysis question?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ3\nWhich organisational priorities does this analysis address?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ4\nIf you use a model, is it business critical?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ5\nWho needs the answer to the analysis question?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ6\nWho do you need to consult to make sure you meet the right user needs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\n\n\n\nQ7\nHow will you know you have answered the analysis question correctly?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\n\n\n\nQ8\nWhat is the estimated time and resource required to answer the analysis question (in months and FTE)?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ9\nWhat is the impact if the analysis is not done now?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ10\nWhat is the impact if the work is not done correctly?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ11\nName the commissioner, senior responsible owner and analytical assurer of this analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ12\nWhat tools and resources will you use in production? Are they the best for the job?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ13\nDo you have the right internal and external resources and capability to deliver the analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ14\nWhat are the anticipated risks of the analysis? Have you discussed these risks with customers and stakeholders?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ15\nIs there a contingency plan prepared if your mitigation plans fail?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ16\nDo the data and analysis comply with ethical requirements?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ17\nWhat relevant questions are outside the scope of the analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ18\nHow will you peer review and assure the work?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ19\nWill external experts be involved in development and scrutiny of analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesign\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ20\nIs there a simple description in plain English of what the analysis is for and what it does?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ21\nDoes the analysis have a logic flowchart which explains the end-to-end conceptual steps in the work flow?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ22\nWhen do you expect to start and finish each stage of analysis i.e., data collection, processing, quality assuarnce, analaysis and dissemination?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ23\nDoes any part of the analysis rely on manual processing? Have you considered the cost and benefits of fully automating the process?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ24\nWhat happens if any of your team members, reviewers or users find a mistake in your analysis? Do you have a clear and efficient process for addressing the concern and preventing it from happening again?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ25\nHave you assessed uncertainty?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing and checking the analysis\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ26\nHow the data used in the analysis will be processed prior to and during use?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ27\nIs the data appropriate given the methods selected?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ28\nWhat are the strengths and limitations of the data that you use?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ29\nIs there a robust relationship between your team and data providers? Does your data provider have a good understanding of how and why you are using their data? Do you have a good understanding of how the data provider collects, processes and quality assures the data?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ30\nIs there a formal agreement in place that specifies when, what and how the data will be received? If not, would this be helpful?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ31\nDo you know what quality checks are carried out on the data before you receive them?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ32\nHow will you work with your data provider when your data requirements change?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ33\nHow do you know if your data provider makes a change to their systems or processes, which could impact the data you receive and/or the statistics you produce?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ34\nHow did you choose the methods for the analysis? How do you know the method you are using is appropriate?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ35\nHave reasonable alternative methods been explored and rejected for good reasons?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ36\nHow do you know that your analysis works correctly?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ37\nCan you describe the assumptions of your analysis, when they were made and who made them and signed them off?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ38\nHow are your assumptions validated and assured prior to use?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ39\nHow do you measure and report uncertainty in your analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ40\nHave you thought about the implications of any unquantified uncertainties?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ41\nCan you explain how your analysis feeds into downstream processes? Are there any risks around these dependencies?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ42\nIs all or part of the analysis reliant on a single person? If yes, how are you mitigating this risk?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ43\nIs it clear why important decisions were made and who made them?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ44\nIf changes are made to code or datasets, can you easily track who made the changes and why the changes were made?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ45\nCould another analyst carry out the analysis and make the outputs just by reading documentation and desk instructions without needing to consult with anybody else?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ46\nDo you use peer review to check scripts and code, documentation, implementation of methods, processes and outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ47\nIs your code and analysis ever peer reviewed by someone outside your team or organisation?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ48\nWhat is your assessment of the quality of your analytical outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ49\nHow do you assure yourselves that analysis carried out is correct?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ50\nDo the outputs of your analysis match reported content?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ51\nIf you find outliers or unusual trends in the data, what steps are taken to investigate them?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelivery\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ52\nCan you give a clear account of what can and cannot be inferred from your final output? Q52 Could you give a clear account of what can and cannot be inferred from your final output?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ53\nHave you assessed the impact of the data and analysis limitations and set out how they will affect the quality and use of the outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ54\nHave you sense checked outputs with user groups and stakeholders?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ55\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ56\nAre the implications of unquantified uncertainties communicated to users?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ57\nIs analysis documentation, including technical guides and code, publicly available?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ58\nDoes the technical guide explain how to use the analysis to obtain valid outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ59\nHave you fully documented the analysis code?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ60\nAre users able to feed back on the suitability of outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text"
  }
]