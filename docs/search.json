[
  {
    "objectID": "quality_questions.html",
    "href": "quality_questions.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nTo get the most out of the template, we strongly recommend that teams identify who will take the key quality assurance roles of Commissioner, Senior Responsible Owner and Analytical Assurer and name the analytical team at the start of the analytical cycle. This is crucial because together these roles help to make sure that the analysis you do is fit-for-purpose."
  },
  {
    "objectID": "quality_questions.html#iv.-delivery",
    "href": "quality_questions.html#iv.-delivery",
    "title": "Quality Questions",
    "section": "IV. Delivery",
    "text": "IV. Delivery\n\nQuality questions and why they matterThe questions and the Code of PracticeLinking the questions to AQuA roles\n\n\n\n\n\n\n\n\nQuality Question\n\n\nWhy do I need to know the answer to this?\n\n\n\n\nQ53\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nOften the aim of final output is to inform decison-making. Output might include predictions, involving lots of underlying assumptions. It is critical that you support your users to make appropriate use of outputs and understand what can and cannot be inferred. Without this, users may misinterpret findings, make in appropriate comparisons, use the analysis for unsuitable purposes and arrive at the wrong conclusions. For example, a non-expert user may wrongly interpret correlation as causation or use incomplete or disconnected data to make forecasts.\n\n\n\n\nQ54\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nYou should describe why limitations related to data and methods exist, why they cannot be overcome using the chosen approach and their impact on the quality and interpretation of the output. Analysis is of very little value if limitations aren’t properly documented and explained.\n\n\n\n\nQ55\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nYou should work with users, experts, and other relevant stakeholders to verify the credibility of outputs and sense check that they are useful.\n\n\n\n\nQ56\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nOutputs are never 100% accurate. Users need to understand how uncertainties related to data, assumptions and methodology feed into and through the analysis workflow and what this means for the use of the outputs. Results must clearly explain how uncertainty affects the findings from the analysis, or we risk misinterpretations and conclusions being overly reliant on imprecise results.\n\n\n\n\nQ57\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nYou must support your users to understand relevant uncertainties which are not captured in the analysis. When you can, make reasonable judgements about the likely size and direction of unquantified uncertainty. Provide a qualitative description informing users about why the uncertainty cannot be quantified and their likely impact.\n\n\n\n\nQ58\n\n\nIs workflow documentation including technical guides and code repositories publicly available?\n\n\nTransparency about your analysis supports proper scrutiny and challenge, promotes public trust, and encourages re-use of the resources you develop.\n\n\n\n\nQ59\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nA good technical guide helps everybody to understand what the analysis does and how it works. A well-written technical guide is essential for effective maintenance of the analysis. It helps users of the analysis to replicate the findings, get answers to methodology questions and build their trust in the output.\n\n\n\n\nQ60\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nThe technical guide is complemented by fully documented analysis code. Code documentation must comply with good practice so new users can understand and execute the code as easily and quickly as possible.\n\n\n\n\nQ61\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nExternal critique makes analysis more robust. Users should be able to give feedback to your team to ensure that results meet their needs. User feedback and customer reviews inform you of issues and changes that you might need to make. They also act as evidence that users have been consulted.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich pillar and principle of Code of Practice matter here?\n*Trustworthiness (T), Quality (Q), Value (V)\n\n\n\n\nQ53\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ54\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.Q1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.Q3.1 Statistics should be produced to a level of quality that meets users’ needs. The strengths and limitations of the statistics and data should be considered in relation to different uses, and clearly explained alongside the statistics.\n\n\n\n\nQ55\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.\n\n\n\n\nQ56\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nQ1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.Q2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ57\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.Q3.3 The extent and nature of any uncertainty in the estimates should be clearly explained.\n\n\n\n\nQ58\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.\n\n\n\n\nQ59\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.V3 Statistics and data should be presented clearly, explained meaningfully and provide authoritative insights that serve the public good.\n\n\n\n\nQ60\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nT4.4 Good business practices should be maintained in the use of resources. Where appropriate, statistics producers should take opportunities to share resources and collaborate to achieve common goals and produce coherent statistics.Q2.1 Methods and processes should be based on national or international good practice, scientific principles, or established professional consensus.\n\n\n\n\nQ61\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nV1 Users of statistics and data should be at the centre of statistical production; their needs should be understood, their views sought and acted upon, and their use of statistics supported. V1.4 Statistics producers should engage publicly through a variety of means that are appropriate to the needs of different audiences and proportionate to the potential of the statistics to serve the public good. An open dialogue should be maintained using proactive formal and informal engagement to listen to the views of new and established contacts. Statistics producers should undertake public engagement collaboratively wherever possible, working in partnership with policy makers and other statistics producers to obtain the views of stakeholders.V1.5 The views received from users, potential users and other stakeholders should be addressed, where practicable. Statistics producers should consider whether to produce new statistics to meet identified information gaps. Feedback should be provided to them about how their needs can and cannot be met, being transparent about reasons for the decisions made and any constraints.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Question\n\n\nWhich AQuA role(s) would normally answer this?\n\n\nWhy are these AQuA roles involved?\n\n\n\n\nQ53\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nCommissioner, analyst\n\n\nDuring the delivery phase, the commissioner receives the results of the analysis and decides whether it meets their needs. The analyst provides sufficient information to support the commissioner to make an informed decision.\n\n\n\n\nQ54\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nCommissioner, analytical assurer, analyst\n\n\nThe commissioner must be confident in the quality of the outputs. They should understand the strengths, limitations and context of the analysis so that the results are correctly interpreted. Analytical assurer sign-off provides confidence that analysis risks, limitations and major assumptions are understood by the users of the analysis. Analysts make sure that the commissioner and analytical assurer have the evidence they need.\n\n\n\n\nQ55\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst and analytical assurer should enable and encourage peer review. Peer reviews provide useful critical challenge about the analytical approach, application of methods and interpretation of the analysis. Verification and peer review of work should be done by analysts who had no involvemen in the work  so their views are independent.\n\n\n\n\nQ56\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nAnalyst, commissioner\n\n\nThe analyst must determine and communicate the uncertainty associated with the analysis so the commissioner can make informed decisions. The commissioner should ensure that an assessment of uncertainty has been provided and that the implications of uncertainty are understood.\n\n\n\n\nQ57\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nAnalyst, commissioner\n\n\nIf uncertainty is too complex to quantify, even approximately, the analysts should explain this so the commissioner can take this into account. In communicating analysis results to decision-makers and stakeholders, the commissioner should be open about the existence of deep uncertainties whose impact cannot be assessed, and explain how they are managed in the analysis.\n\n\n\n\nQ58\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst must produce appropriate design documentation. Best practice includes maintaining a record of the analysis workflow in a technical report, including a concept of analysis, user requirements, design specification, functional specification, data dictionary, and test plan. Code should be properly documented.\n\n\n\n\nQ59\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst must produce appropriate documentation. Best practice includes maintaining a record of the work that has been done in a technical report, including a full description of the analysis, user requirements, design specification, functional specification, data dictionary, and test plan. The analytical assurer makes sure that the documentation is fit for purpose.\n\n\n\n\nQ60\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nAnalyst, analytical assurer\n\n\nAnalysts should develop and maintain analysis code in line with best practice. Code must comply with relevant policies and standards.\n\n\n\n\nQ61\n\n\nIs there a clear feedback mechanism so users can report back on the suitability of outputs?\n\n\nAnalyst, senior responsible owner\n\n\nYou can assess the usefulness of the analysis by getting feedback from users, stakeholders and other experts. Quality analysis should be free of prejudice or bias. The SRO and analysts should check that the analysis follows the principles of RIGOUR (Repeatable, Independent, Grounded in reality, Objective, Uncertainty-managed, Robust)"
  },
  {
    "objectID": "assumptions_and_issues_log.html",
    "href": "assumptions_and_issues_log.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us."
  },
  {
    "objectID": "assumptions_and_issues_log.html#definitions",
    "href": "assumptions_and_issues_log.html#definitions",
    "title": "Quality Questions",
    "section": "Definitions",
    "text": "Definitions\nStart filling in the template by inserting the full name of the analysis (this should align with the name given on the analysis output report) and financial year of interest."
  },
  {
    "objectID": "assumptions_and_issues_log.html#definitions-1",
    "href": "assumptions_and_issues_log.html#definitions-1",
    "title": "Quality Questions",
    "section": "Definitions",
    "text": "Definitions\nStart filling in the template by inserting the full name of the analysis (this should align with the name given on the analysis publications or output report) and financial year of interest."
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Feedback",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\n\n\n\n\nFeedback\nWe are keen to understand the challenges faced by analytical teams in ONS when complying with good practice. To evaluate the effectiveness of this guidance, we would be grateful if you could fill in a short survey.\nWe appreciate your feedback. It will help us to make additional tools and resources on good practice for producing quality analysis.\nIf you have questions about this guidance or the survey please email ASAP@ons.gov.uk “analytical QA” in the subject header.",
    "crumbs": [
      "Feedback"
    ]
  },
  {
    "objectID": "faqs.html",
    "href": "faqs.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us."
  },
  {
    "objectID": "faqs.html#when-should-i-use-this-guidance",
    "href": "faqs.html#when-should-i-use-this-guidance",
    "title": "Frequently Asked Questions",
    "section": "When should I use this guidance?",
    "text": "When should I use this guidance?\nIf you are starting work on a new analysis project we recommend that you use this guidance right from the scoping stage. For teams who are in the middle of an analysis, you can still use the guidance and templates to think about the quality questions from the scoping and design stages and continue recording the answers up until the delivery stage."
  },
  {
    "objectID": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "href": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "title": "Frequently Asked Questions",
    "section": "Which analysis is in scope of the guidance?",
    "text": "Which analysis is in scope of the guidance?\nThe guidance applies to all analysis workflows, including analysis to support research, advice for other departments, ad-hoc and one-off outputs and regular outputs.\nRegular outputs often fulfil at least one of the criteria for what makes a business critical model:\n* Underpins essential financial and funding decisions;\n* Essential to the achievement of business plan actions and priorities;\n* Must be error free or else risk serious financial or legal penalties or reputational damage for the organisation.\nBusiness critical analysis requires the highest level of scrutiny and the governance arrangements must be appropriate for the level of risk.\nFor ad-hoc outputs with quick turnarounds and limited time and resources, analysis still needs sufficient quality assurance to ensure it is fit for purpose. Ad-hoc outputs are often used as part of the evidence when making important policy decisions, so getting them right is very important.\nWe strongly encourage senior responsible owners of both regular and ad-hoc analysis to answer all questions relevant to their role.\nCompleting the logs makes it much easier to understand the risks that the analysis carries and will help you plan to mitigate them. Having a proper audit trail and comprehensive logs also help when:\n* You need to explain why the analysis works as it does.\n* You need to raise quality and resourcing issues and push back against demands that are unrealistic.\n* You need to understand and communicate potential risks."
  },
  {
    "objectID": "faqs.html#how-should-i-identify-the-senior-responsible-owner-commissioner-analyst-and-analytical-assurer",
    "href": "faqs.html#how-should-i-identify-the-senior-responsible-owner-commissioner-analyst-and-analytical-assurer",
    "title": "Frequently Asked Questions",
    "section": "How should I identify the Senior Responsible Owner, Commissioner, Analyst and Analytical Assurer?",
    "text": "How should I identify the Senior Responsible Owner, Commissioner, Analyst and Analytical Assurer?\nThe AQuA Book sets out four roles responsible for the assurance of analysis:\n* Commissioner\n* Senior Responsible Owner (SRO) (usually leads the analyst team)\n* Analysts (who usually report to the SRO) and\n* Analytical Assurer.\nWhat matters here is that each assurance role is covered in your workflow and that individuals know about and accept their responsibilities, not the name of the role.\nAs a project team, you should make sure that each role is in place and you understand how it will operate for you so that you have the right assurance.\nThe AQuA Book makes no expectations about levels of seniority or grade of each of the occupiers of the roles. It does not specify whether roles should be held by a person or could be held by a committee or other governance group (such as a senior leadership team).\nThe key consideration is whether or not the person or group that undertakes the assurance role have the skills and resources they need to meet the requirements of the role. How the roles are covered in an analysis workflow varies from project to project, depending on how the work is planned, managed and assured.\nWe suggest that if roles are taken by individuals, the SRO and commissioner should usually be at Grade 7 or above. If you are still unsure about how to allocate the roles among your team and governance groups, please email us with “analysis assurance” in the subject header and we will help you to make the decision."
  },
  {
    "objectID": "faqs.html#what-help-is-available-to-answer-the-quality-questions",
    "href": "faqs.html#what-help-is-available-to-answer-the-quality-questions",
    "title": "Frequently Asked Questions",
    "section": "What help is available to answer the quality questions?",
    "text": "What help is available to answer the quality questions?\nFor ONS staff, the ONS Quality Central wiki contains lots of useful guidance, templates and mandatory training to help you work through and answer the quality questions. It includes the ONS Quality Standard for Analysis.\nThese cross-government resources are also likely to be useful:\n* Government Data Quality Hub Quality Questions and Red Flags and Government Data Quality Framework\n* Office for Statistics Regulation Quality Assurance of Administrative Data toolkit and Quality and statistics: an OSR perspective.\n* The Uncertainty Toolkit for Analysts in Government"
  },
  {
    "objectID": "assumptions_and_issues_log.html#decisions-log",
    "href": "assumptions_and_issues_log.html#decisions-log",
    "title": "Quality Questions",
    "section": "Decisions log",
    "text": "Decisions log\nThe decisions log is a template to record all the important decisions that are made during the analysis lifecycle, as well as when they were made, who made them and why. Without documentation, decisions may be understood by one part of the team and unknown by others. The decisions log is there to make sure that everybody knows about the decisions that have been made about the analysis. It provides an audit trail of the decisions that were made.\nDecision ID: Give each decision a unique ID so that it can be tracked easily and cross referenced.\nDecision name: Enter a name for the decision.\nDate of decision: Enter the date the decision was made.\nPlain English description of decision A brief summary of the decision in plain English explaining what it was about and how it applies in the analysis.\nName of person or group signing off the decision: Enter the name of the person or group who made the decision. If the decision was made by a committee, link to the minute where the decision is documented.\nRole of person or group signing off the decision: Provide a short description of the person or group’s role in the project. Decisions are often signed off by the project lead or Senior Responsible Owner, for example.\nDate of last review / update: Date the decision was last considered. This is here so you know when the decision was last looked at. Projects evolve, and decisions might need to be reviewed.\nReviewed by: Name(s) of people or group who last reviewed the decision.\nNext review/update due on: Enter the date the decision next needs to be reviewed or updated."
  },
  {
    "objectID": "assumptions_and_issues_log.html#issues-log",
    "href": "assumptions_and_issues_log.html#issues-log",
    "title": "Quality Questions",
    "section": "Issues log",
    "text": "Issues log\nThe issues log provides a standard template to record all the issues incurred during the analysis lifecycle. Without documentation, risks and issues may be well understood by one part of the team and totally unknown by others. The issues log ensures that everybody knows about the issues that the analysis faces. The log helps teams to store problems and challenges for future review and mitigation. It provides an audit trail of past, live and untreated issues.\nIssue ID: Give each issue a unique ID so that it can be tracked easily and cross referenced.\nIssue name: Enter a name for the issue. It could be anything raised by the team, anything raised during quality assurance by the team members or external reviewers. Include the location of the issue, for example, line number of code, error in publication, where it arises in the workflow, resource constraint.\nDate identified: Enter the date the issue was first identified.\nPlain English description of issue: A brief summary of the underlying cause and nature of issue in plain English explaining what is creating problem.\nImpact of issue: Brief summary of how the issue affects the project, for example, timeline, accuracy, cost.\nStatus of issue: From the dropdown menu, select ‘Live’ if the issue is being handled, ‘Resolved’ if the issue has been resolved or ‘Untreated’ if the issue has yet to be handled.\nJustification of status: Brief summary of how the solution has resolved the issue if the status is ‘Resolved’. For ‘Untreated’ issues, explain why the issue will be dealt with later.\nProof of resolution: Write the name of the output report/methodology paper or the published document where the issue is resolved. If the issue relates to code, identify the module and line number.\nDate of last review/update: Enter the date the issue was last reviewed or updated.\nReviewed by: Enter the full name of individual(s) or group who reviewed the issue.\nNext review/update due on: Enter the date the issue next needs to be reviewed or updated."
  },
  {
    "objectID": "assumptions_and_issues_log.html#assumptions-log-1",
    "href": "assumptions_and_issues_log.html#assumptions-log-1",
    "title": "Quality Questions",
    "section": "Assumptions log",
    "text": "Assumptions log\nAssumption ID: Give each assumption a unique ID so that it can be tracked easily and cross referenced.\nLocation in code, documentation or publication: Write the name of the document or publication where the assumption is applied. If the assumption is applied in your code, identify where the assumption applies by citing the module and line number.\nPlain English description of assumption: Write a clear description of the assumption in plain English. For example, “We assume that our sample of income data is representative for the whole population”.\nBasis for assumption: Briefly summarise why the assumption matters for the analysis and why it is reasonable. For example, the assumption could be based on historical data, theoretical requirements of the method, empirical evidence, quality assurance, common practice or data testing.\nNumerical value of the assumption: If you can, attach a numerical value to the assumption. This will depend on what the assumption is and how it is applied in your work. For example, you might make an assumption that counts in your data are accurate to within 5 units, so you would record +/- 5 as the numerical value. Thinking about the value of the assumption in numeric terms is a useful way to work through its impact on your work.\nRange around the estimated value: Assumptions are rarely certain. If you can, assign a range to the central value of the assumption.\nLinks to supporting analysis: Attach a link to the source where the assumption comes from or from where it can be verified and justified.\nDocumentation dependencies: List the project documents dealing with this assumption. Assumptions can have an impact on different stages of the analysis. For example, an assumption about input data if valid or in-valid could impact the model code, project timeline, robustness of the outputs produced by the model. Logging these dependencies will ensure that the relevant plans and documents are updated once an assumption is validated.\nInternally reviewed by: Enter the full name of individual or group who reviewed the assumption.\nDate of last review/update: Enter the date the assumption was last reviewed or validated.\nExternally reviewed by: Enter the name of organisation and the position of the expert who validated the assumption. It is not necessary to mention the individual’s name. For example, Lighthouse Laboratory (Senior Scientist), University of Oxford (Professor of Statistics).\nDate of external review: Enter the date the assumption was reviewed by the external expert.\nNext review/update due on: Enter the date the assumption next needs to be reviewed or updated.\nQuality Rating: See the Quality Rating Key and fill it accordingly.\nRisk Score: See the Risk Score and fill it accordingly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quality Questions Introduction",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us."
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Quality Questions Introduction",
    "section": "Aims",
    "text": "Aims\nThe guidance has five main aims:\n\nTo help analysts understand the level of risk they are carrying in their analytical workflows.\nTo ensure there is a consistent end-to-end QA approach across ONS.\n\nTo make it easier to comply with good practice guidance and standards including the ONS Quality Practices, ONS Quality Standard for Analysis, the government AQUA Book and the Code of Practice for Statistics, the Analysis Function Functional Standard for Analysis and the Government Service Manual which explains how to research, document and validate user needs.\n\nTo ensure there is a consistent understanding of roles and responsibilities when producing high quality analysis and statistics.\n\nTo make it easier to create critical project documentation including an assumptions and decisions log, issue and decisions log, risk register and divisional Quality Improvement Plan.\n\nReflecting on the questions asked in the template will help you to manage your analysis risks:\n\nYou will be able to document the mitigation that is in place or planned.\n\nYou will know which issues and risks the project is prepared to accept and why.\n\nYou can identify potential quality issues and decide how to manage and prioritise them.\n\nHaving this information in once place provides a sound basis for regular reviews of assumptions, issues and risks associated with the workflow, in line with recommended good practice."
  },
  {
    "objectID": "index.html#how-the-questions-draw-on-other-frameworks",
    "href": "index.html#how-the-questions-draw-on-other-frameworks",
    "title": "Quality Questions Introduction",
    "section": "How the questions draw on other frameworks",
    "text": "How the questions draw on other frameworks\nThe AQuA book sets out a standard framework for managing analytical quality in HM Government. AQuA is there to make sure that our work can be trusted to inform good decision making, while the Code of Practice for Statistics sets out the principles and practices that producers of official statistics should commit to.\nTwo other pieces of guidance have motivated us to produce this template. One is the Analysis Function guidance on Quality Questions and Red Flags. The other is the Office for Statistics Regulation (OSR) guidance on Thinking about quality when producing statistics. Both of these provide sets of questions that analysts can use to interrogate their work and assure its quality.\nBuilding on these resources, this guidance sets out quality questions that are relevant for each stage of analytical cycle. The quality questions are at their most effective if they are asked at the right stage. Once that stage is passed, experience suggests that it is normally difficult to go back and address the points the questions ask by retrofitting at a later stage of the analysis.\nEach question is linked with the Code of Practice for Statistics pillars of Trustworthiness, Quality and Value. We explain the importance and relevance of each question in light of the three pillars so teams can better understand and apply these principles through out the project life cycle.\nQuality questions are also linked to which responsible role from the AQuA book would usually answer them. The idea is to highlight the clear line of accountability set out in the AQuA book in an easy-to-understand manner. We want to make it easier for teams to decide how the three key assurance roles of commissioner, senior responsible owner (and their analysis team) and analytical assurer are covered in their own workflows."
  },
  {
    "objectID": "index.html#structure-of-the-guidance",
    "href": "index.html#structure-of-the-guidance",
    "title": "Quality Questions Introduction",
    "section": "Structure of the guidance",
    "text": "Structure of the guidance\nThere are 61 quality questions in total. They cover all the stages of the analytical cycle.\nAnswering 61 questions probably seems like a daunting task at first! The questions are designed to help you as you work your way through the analytical process, rather than to be answered all at once. Creating a log of answers as you move through the stages of the analysis workflow will help you to check that your work meets analytical standards, follows good practice and (if relevant) complies with the Code of Practice for Statistics.\nAnswering the questions will also help you make sure that everybody working on the analysis has a clear understanding of how and why it works as it does, and to support your users when writing your outputs. Moreover, most of the answers will help you to produce the critical documents that mitigate risk like an assumptions log, decisions log, issues log, and technical guides for your team and your users.\nThe AQuA book divides the analytical cycle into four stages:\n\nScoping\n\nDesign\n\nConducting and checking analysis\n\nDelivery\n\nYou can find the questions in the Quality Questions section of the website, which is divided across three themes:\n\na) Quality Questions and why they matter\nThis tab lists the quality questions that are relevant for each stage of analytical cycle. Alongside each question we explain why it matters and explain the potential risks and benefits around it. We try to address the very sensible question of “why should I care about this?”.\nWe encourage commissioner, analyst team and analytical assurer to answer all the questions that are relevant to their responsibilities. We have made a downloadable, editable HTML template that you can use to record your answers to the questions, as well as key project information. Once you have recorded answers, save the HTML file in your project archive. You can update it as you work through the questions.\nUser needs and project aims often change as things evolve, so the HTML template allows you to edit answers by simply changing the text in the editable text boxes. To track these changes, the person writing the answer should include their full-name, the date they have added the answer, and the date the next review is due.\nThe Senior Responsible Owner (SRO) is the lead analyst responsible for the workflow (or relevant part of it). The SRO or their designated representative needs to sign-off all the answers at the top of the sheet.\nIt is best to complete and save the log of answers for each stage of analytical cycle. Answers can be saved by pressing Ctrl + S. To undo an action press Ctrl + Z. Pressing Ctrl + S will let you save a copy of the HTML template on your device.\n\n\nb) The questions and the Code of Practice for Statistics\nThis tab shows the pillars and principles of Code of Practice for Statistics each quality question relates to. The text in this column is copied from Code of Practice for Statistics. Even if your work does not directly feed into the production of official statistics, compliance with the principles and practices of the Code is a good way to strengthen the resilience of your work, increase transparency and clarity and reduce risk.\n\n\nc) Linking the questions to AQuA roles\nThis tab explains the responsibilities required to deliver analysis that is fit-for-purpose. The roles and responsibilities in this tab are reproduced from the AQuA book. The tab links the quality questions with the relevant AQuA Book responsibilities at each stage of analytical cycle.\nThe AQuA book sets out four roles that cover different areas of assurance responsibility. Taken together, they provide a comprehensive set of assurance for an analytical project. The roles are:\n\nCommissioner\n\nSenior Responsible Owner (SRO)\n\nAnalyst\n\nAnalytical assurer\n\nLet’s look at the roles in more detail.\nResponsibilities of the Commissioner\nThe commissioner is focused on making sure the analysis meets the required user needs.\n* Ensures that context around the work is understood so quality assurance is appropriate and proportionate.\n* Ensures that there is enough time and resource for required assurance and account for risk.\n* Must understand strengths and limitations including uncertainty so results are interpreted correctly.\n* Delivers QUALITY OF OUTCOME (“The analysis meets user needs and we understand its limitations”).\nResponsibilities of the Senior Responsible Owner\nThe Senior Responsible Owner (SRO) is accountable for the analytical workflow throughout its lifecycle. The Senior Responsible Owner is usually a senior member of the analytical team, and works closely with (or manages) the analyst team.\nThere is no minimum grade for the SRO role, but they should have the expertise, resources and accountability to ensure that the analysis is well designed, complies with relevant standards, works as intended and is fit for purpose.\n* Signs off all important decisions made about the analysis to ensure that it is fit-for-purpose, prior to use.\n* Delivers QUALITY OF CONTENT (“The analysis is well-designed and uses the right tools and methods to meet user needs”) alongside the analyst team.\nResponsibilities of the Analyst\nThe analyst team usually work to the SRO. Analysts are responsible for setting up, running, checking and reporting on the analysis.\n* Assist the Commissioner and SRO in framing the question to ensure the right analysis is done.\n* Manage external specialists.\n* Design, build, document and run the analysis.\n* Delivers QUALITY OF CONTENT (“The analysis is well-designed and uses the right tools and methods to meet user needs”) alongside the SRO.\nResponsibilities of the Analytical Assurer\nThe analytical assurer is there to make sure that the analysis has the right level of assurance and that the assurance takes place.\n* Ensures that the evidence is there to demonstrate that appropriate quality assurance has happened and that uncertainty is communicated appropriately.\n* Advises the Commissioner on whether appropriate QA has happened and about any outstanding risks.\n* Involved throughout from design through to use.\n* Delivers QUALITY OF PROCESS (“The analysis does what it is supposed to do and we can prove it”).\nYou can read more about the four roles in the AQuA Book and Verification and Validation for the AQuA Book."
  },
  {
    "objectID": "sample_assumptions_log.html",
    "href": "sample_assumptions_log.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us."
  },
  {
    "objectID": "sample_assumptions_log.html#definitions",
    "href": "sample_assumptions_log.html#definitions",
    "title": "Quality Questions",
    "section": "Definitions",
    "text": "Definitions\nAssumptions are ranked red, amber or green (a RAG score) depending on quality. The quality of an assumption measures both how certain and robust an assumption is and how appropriate it is for its intended use.\nFor example, we would usually consider a well documented assumption drawn from published evidence to be very robust, but if it needs to be transformed or adapted significantly to fit the analysis, the quality rating would probably need downgrading. You would normally lower the quality rating if it is not possible to get technical sign-off for the assumption (e.g. because of lack of technical knowledge). You will also want to lower the quality if the confidence interval is likely to be wide (i.e. you wouldn’t be surprised if the value was 50% different from what you measure because of uncertainty).\n\n\n\n\n\n\n\nRAG Rating\nAssumption quality\n\n\n\n\nGREEN\nBased on validated “actual” data; Methodology is robust; No or few transformations, or transformation methodology is fully verified and robust; Data is current and signed off by experts; Confidence intervals are narrow.\n\n\nAMBER\nThe methodology is robust but based on limited data; Data required significant transformation to fit the model; Confidence interval is quite wide; Data has not been reviewed recently.\n\n\nRED\nUnclear/unreliable data source or no data source provided; Based on limited data and methodology not robust; Data is not current; Confidence interval is wide or quality is unknown."
  },
  {
    "objectID": "sample_assumptions_log.html#assumptions---uk-universities-input-dataset",
    "href": "sample_assumptions_log.html#assumptions---uk-universities-input-dataset",
    "title": "Quality Questions",
    "section": "Assumptions - UK universities input dataset",
    "text": "Assumptions - UK universities input dataset\nThis analysis makes the following assumptions\n\n\n\n\n\n\n\n\nAssumption Number\nAssumption description\nQuality Rating\n\n\n\n\n1\nData is representative of the population\nGREEN\n\n\n2\nData does not exclude any population groups based on their demographic and socio-economic characteristics\nGREEN\n\n\n3\nAll UK universities report data to the Higher Education Statistics Agency (HESA)\nRED\n\n\n4\nAll universities accurately report the number of students enrolled during the academic year\nRED\n\n\n5\nAll universities accurately report the number of students who dropped out during the academic year\nRED\n\n\n6\nThe academic year is consistently measured across UK universities\nRED\n\n\n7\nStudents who receive special education services are excluded from the calculation of dropout rates\nRED\n\n\n8\nThere is complete information for all the variables in the analysis\nRED\n\n\n9\nThe data collection process has not changed at all over time\nRED"
  },
  {
    "objectID": "quality_questions.html#quality-questions-and-why-they-matter-2",
    "href": "quality_questions.html#quality-questions-and-why-they-matter-2",
    "title": "Quality Questions",
    "section": "Quality questions and why they matter",
    "text": "Quality questions and why they matter\n\n\n\n\n\nQuality Question\n\n\nWhy do I need to know the answer to this?\n\n\n\n\nQ26\n\n\nHow will the data in the analysis be processed before and during use?\n\n\nProcessing the data inputs will impact methods and outputs. A clear understanding of how these processes affect the workflow is essential for understanding quality.\n\n\n\n\nQ27\n\n\nIs the data appropriate given the methods selected?\n\n\nA comprehensive understanding of data inputs is a prerequisite to meeting user needs.\n\n\n\n\nQ28\n\n\nWhat are the strengths and limitations of the data you use?\n\n\nData requirements for analysis vary. Formats, coverage, time scales and granularity must all be appropriate for the research question.A comprehensive understanding of data inputs is a prerequisite for meeting user needs. Without understanding the strengths and weaknesses of the data, it is impossible to make meaningful improvements to the analysis or the inputs to manage these limitations.\n\n\n\n\nQ29\n\n\nIs there a robust relationship between your team and data providers?Do data providers understand how and why you use their data?\n\n\nA good relationship with data suppliers helps to make sure that their data meet your requirements. Lack of communication can mean you are not aware of quality risks or changes in collection or processing steps that can affect your results.You should communicate with your suppliers sufficiently to manage input quality. Data providers should have a good understanding of how and why you are using their data. This helps them to improve data quality and value and find and address gaps or issues that are relevant for your analysis.\n\n\n\n\nQ30\n\n\nDo you understand how data providers collect, process and quality assure the data you use?\n\n\nNever assume that datasets are of sufficient quality. Make sure that suppliers give you the metadata and other supporting information you need to assure the quality of the data. Validate the information provided by suppliers using your own checks and confirmation if appropriate.\n\n\n\n\nQ31\n\n\nIs there a formal agreement to set out data content, when and how you will get the data? If not, why not?\n\n\nA formal service level agreement with data providers makes sure that everybody understands what will be delivered, when and how. This is useful for setting out the division of responsibilities between data providers and your team for getting and sharing the data. It might specify formats, delivery, timescale, legal framework, accompanying metadata and quality checks.\n\n\n\n\nQ32\n\n\nDo you know what quality checks are carried out on the data before you receive them?\n\n\nData suppliers should be able to show that their data is sufficiently assured to meet your needs. You should be able to demonstrate that the data meet your needs and that reported quality matches what you observe in practice. Simply having a quality report is not enough.\n\n\n\n\nQ33\n\n\nHow will you work with your data provider when your data requirements change?\n\n\nReview your data requirements regularly to ensure they are still relevant and feasible. Changes to requirements should be communicated to data providers well in advance and agreed by all stakeholders. If there is a formal agreement, it may need to be revised as requirements change.\n\n\n\n\nQ34\n\n\nHow do you know if your data provider changes their systems or processes in a way that could impact the data you receive or the analysis you produce?\n\n\nData suppliers make changes to their definitions, methods and systems. This may not affect the quality of their data but can affect how you process the data and what you can infer from it. Tailor communication with data suppliers so it is sufficiently frequent, effective and ongoing to get timely information about changes.\n\n\n\n\nQ35\n\n\nHow did you choose the methods for the analysis? How do you know the methods you use are appropriate?\n\n\nYou should be able to explain why you chose your methods. For each method, document the underlying assumptions, why the method is suitable for answering the analysis question, why it is applicable to the type and distribution of data you are using, and how these decisions were signed off.\n\n\n\n\nQ36\n\n\nHave reasonable alternative methods been explored and rejected for good reasons?\n\n\nThere is often more than one way to answer a question with data. When you have made choices about methods and approaches, explain how and why you considered and rejected other options. Unless there is evidence underpinning your choice, users cannot be sure that you have chosen the most suitable methods.\n\n\n\n\nQ37\n\n\nHow do you know that your analysis is working correctly?\n\n\nYou need to be sure that your analysis produces the outputs you think it should and the processes run as expected. If you cannot demonstrate that scripts and processes work correctly, you cannot confirm the quality of the results.\n\n\n\n\nQ38\n\n\nCan you describe the assumptions of your analysis, when they were made and who made them and signed them off?\n\n\nYou must understand the assumptions your analysis makes. Assumptions set out how the analysis simplifies the world and mitigates uncertainty. If assumptions are inadequately set out or absent, important characteristics of the analysis and its inputs will be unclear, greatly increasing risk.Without a comprehensive log of assumptions made by the analysis, an audit trail signed off by assumption owners, a version control log reflecting when assumptions were last updated, and evidence showing internal and external validation of assumptions, uncertainties may go unacknowledged and could drastically impact outputs.\n\n\n\n\nQ39\n\n\nHow are assumptions validated and assured before you apply them?\n\n\nA clear understanding of how assumptions have been externally and internally validated and signed off gives us confidence that they are reasonable.\n\n\n\n\nQ40\n\n\nHow do you measure and report uncertainty in your analysis?\n\n\nAll analysis contains uncertainty. Quantifying and reporting uncertainty means we can inform users how precise reported values are and how much confidence they can have in the analysis. It will also help you to determine where the analysis can be improved.There are many ways to quantify uncertainty in input data, assumptions, processes and outputs. Choose appropriate ones for your situation. For instance, uncertainties can be understood and quantified by comparing against similar or historical data, Monte Carlo simulation, break-even analysis or using expert judgement.\n\n\n\n\nQ41\n\n\nHave you considered the implications of relevant, unquantified uncertainties?\n\n\nA good understanding of the uncertainties in the analysis workflow is critical to ensure the analysis and its outputs are fit for purpose.\n\n\n\n\nQ42\n\n\nCan you explain the impact of your analysis on downstream processes? Are there any risks around these dependencies?\n\n\nUnderstanding how your analysis might be used would help ensure that the right quality and assurance levels are in place. It would help you assess the risks around the use of analysis and if there are other stakeholders or users you need to consult.\n\n\n\n\nQ43\n\n\nIs all or part of the analysis reliant on a single person?\n\n\nSingle points of failure carry significant business risk. If only one person understands how to carry out all or part of the analysis or maintain the code then the process is extremely vulnerable.\n\n\n\n\nQ44\n\n\nIs it clear why important decisions about the analysis were made, who made them and when?\n\n\nAll analysis involves decisions. A comprehensive record of the decisions made in specifying and conducting the analysis ensures a full audit trail of why decisions were made, who made them and signed them off.\n\n\n\n\nQ45\n\n\nIf changes need to be made to code or datasets, is it easy to track who made the changes and when and why they were made?\n\n\nGood version control ensures a full understanding of when, why, and how changes were made to your analysis process. If it is hard to track changes, it will be hard to retrace steps if there is a problem and means you do not fully understand the process\n\n\n\n\nQ46\n\n\nWould another analyst be able to reproduce your analysis output or continue the work without talking to you first?\n\n\nYour analysis must be well documented and repeatable so that somebody new can understand it, use it, and produce the same output with the same inputs. Poor documentation can lead to errors.\n\n\n\n\nQ47\n\n\nDo you use internal peer review to check scripts and code, documentation, implementation of methods, processes and outputs?\n\n\nYou should independently review and validate the logical integrity of you analysis as well as the structure and functionality of the code against the research question.A record of validation and verification activities undertaken, outstanding tasks and remedial actions helps to confirm that the correct analysis has been performed for the required purpose and the chosen approach minimises risk.\n\n\n\n\nQ48\n\n\nIs your code and analysis ever peer reviewed by someone outside your team or organisation?\n\n\nExternal peer review is one of the best ways to ensure that the analysis and code are well made and fit for purpose. Without it, teams can reinforce their own biases and may not notice there is anything wrong.\n\n\n\n\nQ49\n\n\nWhat is your assessment of the quality of your analytical outputs?\n\n\nUnderstanding and reporting on the quality of your analysis is critical to ensure fitness for purpose and maintain trust and reliability. This ensures that analysis can appropriately inform decision-making. Quality assessments are key information to share with users and a requirement of the Code of Practice for Statistics.\n\n\n\n\nQ50\n\n\nHow do you assure yourselves that analysis you do is correct?\n\n\nIf you check that results fit with your expectations and you can explain discrepancies, this makes it easier to mitigate risk.There are many ways to check if analysis is carried out correctly. For example, sensitivity analysis can help you understand which inputs have the greatest effect on the outputs.You can also compare figures from the analysis with similar data from other sources or from historical series.\n\n\n\n\nQ51\n\n\nDo the outputs of your analysis align with similar findings from elsewhere? If not, can you explain why?\n\n\nIf you can, check that your outputs align with findings from previous runs of the analysis, alternate data sources, and comparable studies. This gives you confidence that the analysis works as expected. You should be able to explain any inconsistencies that you see.\n\n\n\n\nQ52\n\n\nIf you find outliers or unusual trends in the data, what steps do you take to investigate them?\n\n\nIt is crucial to check unusual trends and values in the data and understand why they are there. Not all outliers are the same. Some have a strong influence, some not at all. Some are valid and important data values. Others might be errors.Investigate outliers and unusual patterns thoroughly and take reasonable steps to check their impact on your final output. If you choose to exclude unusual values, you should explain why this is acceptable.",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "quality_questions.html#the-questions-and-the-code-of-practice-2",
    "href": "quality_questions.html#the-questions-and-the-code-of-practice-2",
    "title": "Quality Questions",
    "section": "The questions and the Code of Practice",
    "text": "The questions and the Code of Practice\n\n\n\n\n\nQuality Question\n\n\nWhich pillar and principle of Code of Practice are relevant here?\n*Trustworthiness (T), Quality (Q), Value (V)\n\n\n\n\nQ26\n\n\nHow will the data used in the analysis be processed before and during use?\n\n\nQ1 Statistics should be based on the most appropriate data to meet intended uses. The impact of any data limitations for use should be assessed, minimised and explained.\n\n\n\n\nQ27\n\n\nIs the data appropriate given the methods selected?\n\n\nQ1 Statistics should be based on the most appropriate data to meet intended uses. The impact of any data limitations for use should be assessed, minimised and explained.\n\n\n\n\nQ28\n\n\nWhat are the strengths and limitations of the data you use?\n\n\nQ1 Statistics should be based on the most appropriate data to meet intended uses. The impact of any data limitations for use should be assessed, minimised and explained. Q1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.\n\n\n\n\nQ29\n\n\nIs there a robust relationship between your team and data providers?Do data providers understand how and why you use their data?\n\n\nQ1.2 Statistics producers should establish and maintain constructive relationships with those involved in the collection, recording, supply, linking and quality assurance of data, wherever possible.\n\n\n\n\nQ30\n\n\nDo you understand how data providers collect, process and quality assure the data you use?\n\n\nQ1.1 Statistics should be based on data sources that are appropriate for the intended uses. The data sources should be based on definitions and concepts that are suitable approximations of what the statistics aim to measure, or that can be processed to become suitable for producing the statistics. Q1.6 The causes of limitations in data sources should be identified and addressed where possible. Statistics producers should be open about the extent to which limitations can be overcome and the impact on the statistics.\n\n\n\n\n\nQ31\n\n\nIs there a formal agreement to set out data content, when and how you will get the data? If not, why not?\n\n\nQ1.3 A clear statement of data requirements should be shared with the organisations that provide that data, setting out decisions on timing, definitions and format of data supply, and explaining how and why the data will be used.\n\n\n\n\nQ32\n\n\nDo you know what quality checks are carried out on the data before you receive them?\n\n\nQ1 Statistics should be based on the most appropriate data to meet intended uses. The impact of any data limitations for use should be assessed, minimised and explained.\n\n\n\n\nQ33\n\n\nHow will you work with your data provider when your data requirements change?\n\n\nQ1.2 Statistics producers should establish and maintain constructive relationships with those involved in the collection, recording, supply, linking and quality assurance of data, wherever possible.\n\n\n\n\nQ34\n\n\nHow do you know if your data provider changes their systems or processes in a way that could impact the data you receive or the analysis you produce?\n\n\nQ1.2 Statistics producers should establish and maintain constructive relationships with those involved in the collection, recording, supply, linking and quality assurance of data, wherever possible.Q1.7 The impact of changes in the circumstances and context of a data source on the statistics over time should be evaluated. Reasons for any lack of consistency and related implications for use should be clearly explained to users.\n\n\n\n\nQ35\n\n\nHow did you choose the methods for the analysis? How do you know the methods you use are appropriate?\n\n\nQ2 Producers of statistics and data should use the best available methods and recognised standards, and be open about their decisions.\n\n\n\n\nQ36\n\n\nHave reasonable alternative methods been explored and rejected for good reasons?\n\n\nQ2 Producers of statistics and data should use the best available methods and recognised standards, and be open about their decisions.\n\n\n\n\nQ37\n\n\nHow do you know that your analysis is working correctly?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.\n\n\n\n\nQ38\n\n\nCan you describe the assumptions of your analysis, when they were made and who made them and signed them off?\n\n\nTransparency means being clear and open about the choices you make and not holding back or being opaque about decisions.Q1.6 The causes of limitations in data sources should be identified and addressed where possible. Statistics producers should be open about the extent to which limitations can be overcome and the impact on the statistics.Q2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ39\n\n\nHow are assumptions validated and assured before you apply them?\n\n\nQ1.6 The causes of limitations in data sources should be identified and addressed where possible. Statistics producers should be open about the extent to which limitations can be overcome and the impact on the statistics.Q2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.Q3.5 Systematic and periodic reviews on the strengths and limitations in the data and methods should be undertaken. Statistics producers should be open in addressing the issues identified and be transparent about their decisions on whether to act.\n\n\n\n\nQ40\n\n\nHow do you measure and report uncertainty in your analysis?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ41\n\n\nHave you considered the implications of relevant, unquantified uncertainties?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.Q3.3 The extent and nature of any uncertainty in the estimates should be clearly explained.\n\n\n\n\nQ42\n\n\nCan you explain the impact of your work/output on downstream processes? Are there any risks around these dependencies?\n\n\nV1.1 Statistics producers should maintain and refresh their understanding of the use and potential use of the statistics and data. They should consider the ways in which the statistics might be used and the nature of the decisions that are or could be informed by them.\n\n\n\n\nQ43\n\n\nAll or part of the analysis is reliant on a single person?\n\n\nV4 Statistics producers should be creative and motivated to improve statistics and data, recognising the potential to harness technological advances for the development of all parts of the production and dissemination process.T4.3 Sufficient human, financial and technological resources should be provided to deliver statistical services that serve the public good.\n\n\n\n\nQ44\n\n\nIs it clear why important decisions were made and who made them?\n\n\nQ2 Producers of statistics and data should use the best available methods and recognised standards, and be open about their decisions.\n\n\n\n\nQ45\n\n\nIf changes need to be made to code or datasets, is it easy to track who made the changes and when and why they were made?\n\n\nV4 Statistics producers should be creative and motivated to improve statistics and data, recognising the potential to harness technological advances for the development of all parts of the production and dissemination process.\n\n\n\n\nQ46\n\n\nWould another analyst be able to reproduce your analysis output or continue the work without talking to you first?\n\n\nV5 Statistics and data should be published in forms that enable their reuse. Producers should use existing data wherever possible and only ask for more where justified.\n\n\n\n\nQ47\n\n\nDo you use internal peer review to check scripts and code, documentation, implementation of methods, processes and outputs?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.T4.6 Independent measures, such as internal and external audit, peer review and National Statistics Quality Reviews, should be used to evaluate the effectiveness of statistical processes. Statistics producers should be open about identified areas for improvement.\n\n\n\n\nQ48\n\n\nIs your code and analysis ever peer reviewed by someone outside your team or organisation?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.T4.6 Independent measures, such as internal and external audit, peer review and National Statistics Quality Reviews, should be used to evaluate the effectiveness of statistical processes. Statistics producers should be open about identified areas for improvement.\n\n\n\n\nQ49\n\n\nWhat is your assessment of the quality of your analytical outputs?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.Q3.2 Quality assurance arrangements should be proportionate to the nature of the quality issues and the importance of the statistics in serving the public good. Statistics producers should be transparent about the quality assurance approach taken throughout the preparation of the statistics. The risk and impact of quality issues on statistics and data should be minimised to an acceptable level for the intended uses.\n\n\n\n\nQ50\n\n\nHow do you assure yourself that the analysis you do is correct?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.Q3.2 Quality assurance arrangements should be proportionate to the nature of the quality issues and the importance of the statistics in serving the public good. Statistics producers should be transparent about the quality assurance approach taken throughout the preparation of the statistics. The risk and impact of quality issues on statistics and data should be minimised to an acceptable level for the intended uses.\n\n\n\n\nQ51\n\n\nDo the outputs of your analysis align with similar findings from elsewhere? If not, can you explain why?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.\n\n\n\n\nQ52\n\n\nIf you find outliers or unusual trends in the data, what steps do you take to investigate them?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.Q3.3 The quality of the statistics and data, including their accuracy and reliability, coherence and comparability, and timeliness and punctuality, should be monitored and reported regularly. Statistics should be validated through comparison with other relevant statistics and data sources. The extent and nature of any uncertainty in the estimates should be clearly explained.",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "quality_questions.html#linking-the-questions-to-aqua-roles-2",
    "href": "quality_questions.html#linking-the-questions-to-aqua-roles-2",
    "title": "Quality Questions",
    "section": "Linking the questions to AQuA roles",
    "text": "Linking the questions to AQuA roles\n\n\n\n\n\nQuality Question\n\n\nWhich AQuA role(s) would normally answer this?\n\n\nWhy are these AQuA roles involved?\n\n\n\n\nQ26\n\n\nHow will the data used in the analysis be processed before and during use?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst should collect and manage the data. They must understand data accuracy and uncertainties and capture, manage and understand assumptions. Analytical assurer should check that data processing is sufficient to ensure fitness for purpose.\n\n\n\n\nQ27\n\n\nIs the data appropriate given the methods selected?\n\n\nAnalyst, analytical assurer, commissioner\n\n\nThe analyst should understand data accuracy and uncertainties and capture, manage and understand assumptions made. The analyst should engage appropriate subject matter experts at the appropriate time. The commissioner may be a subject matter expert. The analytical assurer should check that there is sufficient assurance around the choice of data.\n\n\n\n\nQ28\n\n\nWhat are the strengths and limitations of the data you use?\n\n\nAnalyst, analytical assurer\n\n\nIf applicable, analyst should undertake parametric analysis to understand the consequences of missing or uncertain data and assumptions. Analytical assurer should make sure there is sufficient consideration of strengths and limitations of data.\n\n\n\n\nQ29\n\n\nIs there a robust relationship between your team and data providers?Do data providers understand how and why you use their data?Do you understand how data providers collect, process and quality assure the data you use?\n\n\nAnalyst, analytical assurer\n\n\nThe analytical assurer should expect to see evidence that there has been sufficient dialogue between analysts and the providers of data and other evidence sources.\n\n\n\n\nQ30\n\n\nDo you understand how data providers collect, process and quality assure the data you use?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst should ensure data formats, units, and context are properly understood and handled. They should design and implement quality checks to validate data inputs as required. Analytical assurer should verify that the right assurance is in place.\n\n\n\n\nQ31\n\n\nIs there a formal agreement to set out data content, when and how you will get the data? If not, why not?\n\n\nCommissioner, analyst, analytical assurer\n\n\nThe commissioner may need to provide the analyst with agreement to use specific data. The analyst should ensure data formats, units, and context are properly understood and handled. Analytical assurer should verify that assurance is in place.\n\n\n\n\nQ32\n\n\nDo you know what quality checks are carried out on the data before you receive them?\n\n\nAnalyst\n\n\nThe analyst should understand data accuracy and uncertainties and capture, manage and understand implicit assumptions made. The analytical assurer should assess whether assurance is sufficient.\n\n\n\n\nQ33\n\n\nHow will you work with your data providers when your data requirements change?\n\n\nCommissioner, analyst, analytical assurer\n\n\nDuring the design and conduct of analysis, the commissioner may need to provide the analyst with information, agreement to use resources or confirmation of assumptions or approach. The analyst should understand data accuracy and uncertainties and capture, manage and understand implicit assumptions made. The analytical assurer checks that assurance and mitigation are sufficient.\n\n\n\n\nQ34\n\n\nHow do you know if your data provider changes their systems or processes in a way that could impact the data you receive or the analysis you produce?\n\n\nCommissioner, analyst, analytical assurer\n\n\nDuring the design and conduct of analysis, the commissioner provides the analyst with the information they need for the analysis to proceed. This could include agreement to use datasets, setting out of key assumptions and signing off assumptions developed during the project. Analyst should understand data accuracy and uncertainties and capture, manage and understand assumptions made. The analytical assurer checks that assurance and mitigation are sufficient.\n\n\n\n\nQ35\n\n\nHow did you choose the methods for the analysis? How do you know the methods you use are appropriate?\n\n\nAnalyst, analytical assurer\n\n\nDuring the design phase, the analyst will convert the commission into an analytical plan and will consider inputs, analytical methods and processes, and expected outputs. The analytical assurer should check that the proposed design meets the commissioner’s requirements and is sufficiently assured.\n\n\n\n\nQ36\n\n\nHave reasonable alternative methods been explored and rejected for good reasons?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst should review the analysis as a whole and consider carefully whether there are other, better ways in which it could be done. The analytical assurer should check that the investigation of methods was sufficiently thorough and proportionate.\n\n\n\n\nQ37\n\n\nHow do you know that your analysis is working correctly?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst should validate that the analysis as set up to answer the specification of the commissioner. The analytical assurer checks that assurance and mitigation are sufficient so the analysis is fit for purpose.\n\n\n\n\nQ38\n\n\nCan you describe the assumptions of your analysis, when they were made and who made them and signed them off?\n\n\nAnalyst, analytical assurer, commissioner\n\n\nThe analyst should capture, manage and understand explicit and implicit assumptions made. The analytical assurer should assess whether these are sufficient. The commissioner should be made aware of key assumptions and confirm that they are happy that the assumptions are applied.\n\n\n\n\nQ39\n\n\nHow are assumptions validated and assured before you apply them?\n\n\nAnalyst, analytical assurer\n\n\nIf applicable, analyst should undertake parametric analysis to understand the consequences of missing or uncertain assumptions. Analytical assurer should check that validation and assurance of assumptions is sufficient.\n\n\n\n\nQ40\n\n\nHow do you measure and report uncertainty in your analysis?\n\n\nAnalyst, commissioner, analytical assurer\n\n\nAnalyst should determine and communicate the uncertainty associated with outputs so that the commissioner can make informed decisions. The range of possible outcomes and their relative likelihoods should be described. The analytical assurer checks that measuring and reporting of uncertainty is sufficient to meet the needs of the commissioner.\n\n\n\n\nQ41\n\n\nHave you considered the implications of relevant, unquantified uncertainties?\n\n\nAnalyst, commissioner\n\n\nIf uncertainties are too complex for analysts to quantify, even approximately, the analysts should say so in order that the commissioner can take this into account.\n\n\n\n\nQ42\n\n\nCan you explain the impact of your analysis on downstream processes? Are there risks around these dependencies?\n\n\nAnalyst, analytical assurer\n\n\nAnalyst should make sure that the implications of data dependencies or relationships to other analysis and methods are understood. Analytical assurer should check that dependencies have been properly considered.\n\n\n\n\nQ43\n\n\nIs all or part of the analysis reliant on a single person?\n\n\nAnalytical assurer, analyst, commissioner\n\n\nAnalysis should be peer reviewed at an appropriate and proportionate level by a competent person. Comissioner, analyst and analytical assurer should all be involved in each stage of the analytical cycle.\n\n\n\n\nQ44\n\n\nIs it clear why important decisions about the analysis were made, who made them and when?\n\n\nAnalytical assurer, analyst\n\n\nThe analytical assurer should make sure that a suitable audit trail is in place that clarifies the level of validation, scope, and risks associated with the analysis. Best practice includes the production of validation log books. Analyst should build this audit trail.\n\n\n\n\nQ45\n\n\nIf changes need to be made to code or datasets, is it easy to track who made the changes and when and why they were made?\n\n\nAnalytical assurer, analyst\n\n\nTo make analytical audit easy, you should set up a version control system for the analysis as a whole and for code, supporting data and assumptions. Best practice includes the production of validation log books. Analyst should build this audit trail.The Analytical assurer should ensure that the audit trail clarifies the level of validation, scope, and risks associated with the analysis.\n\n\n\n\nQ46\n\n\nWould another analyst be able to reproduce your analysis output or continue the work without talking to you first?\n\n\nAnalyst, analytical assurer\n\n\nGood quality analysis is reproducible. Analyst should check that the analytical process reflects the principles of RIGOUR (Repeatable, Independent, Grounded in reality, Objective, Uncertainty-managed, Robust)\n\n\n\n\nQ47\n\n\nDo you use internal peer review to check scripts and code, documentation, implementation of methods, processes and outputs?\n\n\nAnalyst, analyical assurer\n\n\nThe analyst should provide proportionate documentation that explains the verification and validation activities that the analysis is subjected to. Analysts must perform appropriate test to check the analysis. They should commission other verification and validation as required. Analytical assurer should confirm that planned validation and verification are sufficient.\n\n\n\n\nQ48\n\n\nIs your code and analysis ever peer reviewed by someone outside your team or organisation?\n\n\nAnalyst, analytical assurer\n\n\nAnalysts should work with the commissioner to set out the analysis question so that appropriate analysis is done. Some analysis may require external specialists, so analysts may have responsibilities as part of the procurement process. Analysts, including 3rd parties providing analysis, should provide proportionate documentatiob describing the verification and validation activities undertaken and associated conclusions. The analytical assurer advises the commissioner on whether appropriate analytical quality assurance has taken place.\n\n\n\n\nQ49\n\n\nWhat is your assessment of the quality of your analytical outputs?\n\n\nCommissioner, analytical assurer\n\n\nAs part of the delivery phase, the commissioner should ensure there is an assessment of the level of analytical quality assurance of the analysis, noting where there have been trade-offs between time, resources and quality. The analytical assurer advises the commissioner on whether appropriate analytical quality assurance has taken place.\n\n\n\n\nQ50\n\n\nHow do you assure yourself that the analysis you do is correct?\n\n\nCommissioner, analyst, analytical assurer\n\n\nThe analyst must build in checks and processes to ensure that the analysis is correct. During the delivery phase, the commissioner should give feedback to assist in the correct interpretation of results and determine if the analysis has addressed the commission. The analyst should work with the analytical assurer while doing the analysis so that they can comment on whether the analysis meets the needs of the commission to ensure best use of the results\n\n\n\n\nQ51\n\n\nDo the outputs of your analysis align with similar findings from elsewhere? If not, can you explain why?\n\n\nCommissioner, analyst\n\n\nWhen interpreting the results of a piece of analysis, the commissioner provides constructive challenge. They work with the analyst to explore whether further analysis is needed.\n\n\n\n\nQ52\n\n\nIf you find outliers or unusual trends in the data, what steps do you take to investigate them?\n\n\nAnalyst, analytical assurer\n\n\nIf applicable, analyst should undertake parametric analysis to understand the consequences of missing or uncertain data and assumptions. The analysis plan should include treatment of unusual values and outliers. Analytical assurer should be involved.\n\n\n\n:::",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "index.html#who-is-this-guidance-for",
    "href": "index.html#who-is-this-guidance-for",
    "title": "Quality Questions Introduction",
    "section": "Who is this guidance for?",
    "text": "Who is this guidance for?\nThis guidance provides a set of questions to help analytical and statistical teams evaluate the quality of their analysis throughout the production cycle.\nThe guidance is here to support teams in meeting the Office for National Statistics’s (ONS) strategic objectives for improving statistical quality. You can find more information about our strategic objectives on statistical quality in the ONS Statistical Quality Improvement Strategy. ONS manages quality through a strategic risk approach.\nWe have made the guidance available on Github in case others wish to use the Quality Questions resource in their own work."
  },
  {
    "objectID": "index.html#structure-of-the-quality-questions",
    "href": "index.html#structure-of-the-quality-questions",
    "title": "Quality Questions Introduction",
    "section": "Structure of the quality questions",
    "text": "Structure of the quality questions\nThere are 61 quality questions in total. They cover all the stages of the analytical cycle.\nAnswering 61 questions must seem quite daunting at first! The questions are designed to help you as you work your way through the analytical process, rather than to be answered all at once. Creating a log of answers as you move through the stages of the analysis workflow will help you to check that your work meets analytical standards, follows good practice and (if relevant) complies with the Code of Practice for Statistics.\nAnswering the questions will also help you make sure that everybody working on the analysis has a clear understanding of how and why it works as it does, and to support your users when writing your outputs. Moreover, most of the answers will help you to produce the critical documents that mitigate risk like an assumptions log, decisions log, issues log, and technical guides for your team and your users.\nThe AQuA book divides the analytical cycle into four stages:\n\nScoping\n\nDesign\n\nConducting and checking analysis\n\nDelivery\n\nYou can find the questions in the Quality Questions section of the website, which is divided across three themes:\n\na) Quality Questions and why they matter\nThis tab lists the quality questions that are relevant for each stage of analytical cycle. Alongside each question we explain why it matters and explain the potential risks and benefits around it. We try to address the very sensible question of “why should I care about this?”.\nWe encourage commissioner, analyst team and analytical assurer to answer all the questions that are relevant to their responsibilities. We have made a downloadable, editable HTML template that you can use to record your answers to the questions, as well as key project information. Once you have recorded answers, save the HTML file in your project archive. You can update it as you work through the questions.\nUser needs and project aims often change as things evolve, so the HTML template allows you to edit answers by simply changing the text in the editable text boxes. To track these changes, the person writing the answer should include their full-name, the date they have added the answer, and the date the next review is due.\nThe Senior Responsible Owner (SRO) is the lead analyst responsible for the workflow (or relevant part of it). The SRO or their designated representative needs to sign-off all the answers at the top of the Quality Questions sheet.\nIt is best to complete and save the log of answers for each stage of analytical cycle. Answers can be saved by pressing Ctrl + S. To undo an action press Ctrl + Z. Pressing Ctrl + S will let you save a copy of the HTML template on your device. We recommend uploading the template to your main documentation area and reviewing and updating it as necessary.\n\n\nb) The questions and the Code of Practice for Statistics\nThis tab shows the pillars and principles of Code of Practice for Statistics each quality question relates to. The text in this column is copied from Code of Practice for Statistics. Even if your work does not directly feed into the production of official statistics, compliance with the principles and practices of the Code is a good way to strengthen the resilience of your work, increase transparency and clarity and reduce risk.\n\n\nc) Linking the questions to AQuA roles\nThis tab explains the responsibilities required to deliver analysis that is fit-for-purpose. The roles and responsibilities in this tab are reproduced from the AQuA book. The tab links the quality questions with the relevant AQuA Book responsibilities at each stage of analytical cycle.\nThe AQuA book sets out four roles that cover different areas of assurance responsibility. Taken together, they provide a comprehensive set of assurance for an analytical project. The roles are:\n\nCommissioner\n\nSenior Responsible Owner (SRO)\n\nAnalyst\n\nAnalytical assurer\n\nLet’s look at the roles more detail.\nResponsibilities of the Commissioner\nThe commissioner is focused on making sure the analysis meets the required user needs.\n* Ensures that context around the work is understood so quality assurance is appropriate and proportionate.\n* Ensures that there is enough time and resource for required assurance and account for risk.\n* Must understand strengths and limitations including uncertainty so results are interpreted correctly.\n* Delivers QUALITY OF OUTCOME (“The analysis meets user needs and we understand its limitations”).\nResponsibilities of the Senior Responsible Owner\nThe Senior Responsible Owner (SRO) is accountable for the analytical workflow throughout its lifecycle. The Senior Responsible Owner is usually a senior member of the analytical team, and works closely with (or manages) the analyst team.\nThere is no minimum grade for the SRO role, but they should have the expertise, resources and accountability to ensure that the analysis is well designed, complies with relevant standards, works as intended and is fit for purpose.\n* Signs off all important decisions made about the analysis to ensure that it is fit-for-purpose, prior to use.\n* Delivers QUALITY OF CONTENT (“The analysis is well-designed and uses the right tools and methods to meet user needs”) alongside the analyst team.\nResponsibilities of the Analyst\nThe analyst team usually work to the SRO. Analysts are responsible for setting up, running, checking and reporting on the analysis.\n* Assist the Commissioner and SRO in framing the question to ensure the right analysis is done.\n* Manage external specialists.\n* Design, build, document and run the analysis.\n* Delivers QUALITY OF CONTENT (“The analysis is well-designed and uses the right tools and methods to meet user needs”) alongside the SRO.\nResponsibilities of the Analytical Assurer\nThe analytical assurer is there to make sure that the analysis has the right level of assurance and that the assurance takes place.\n* Ensures that the evidence is there to demonstrate that appropriate quality assurance has happened and that uncertainty is communicated appropriately.\n* Advises the Commissioner on whether appropriate QA has happened and about any outstanding risks.\n* Involved throughout from design through to use.\n* Delivers QUALITY OF PROCESS (“The analysis does what it is supposed to do and we can prove it”).\nYou can read more about the four roles in the AQuA Book and Verification and Validation for the AQuA Book."
  },
  {
    "objectID": "sample_assumptions_table.html",
    "href": "sample_assumptions_table.html",
    "title": "Sample assumptions log",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nThis log contains a list of assumptions used in an example analysis of data from UK universities. It also provides a score for each assumption."
  },
  {
    "objectID": "sample_assumptions_table.html#definitions",
    "href": "sample_assumptions_table.html#definitions",
    "title": "Sample assumptions log",
    "section": "Definitions",
    "text": "Definitions\nAssumptions are scored as Red, Amber or Green (a RAG rating) depending on quality. In RAG, green denotes a favourable value, red unfavourable and amber neutral. The quality of an assumption measures both how certain and robust an assumption is and how appropriate it is for its intended use.\nFor example, we would usually consider a well documented assumption drawn from published evidence to be very robust, but if it needs to be transformed or adapted significantly to fit the analysis, the quality rating might need downgrading.\nYou would normally lower the quality rating of an assumption if you cannot get technical sign-off (for example because of lack of technical knowledge) or if the information on which it is based is incomplete or poor quality. You would also normally lower the quality if the confidence interval or uncertainty range is wide (i.e. you wouldn’t be surprised if the value was 50% different from what you measure because of uncertainty).\n\n\n\n\n\n\n\nRAG Rating\nAssumption quality\n\n\n\n\nGREEN\nBased on validated data; Methodology is robust; No or few transformations, or transformation methodology is fully verified and robust; Data is current and signed off by experts; Confidence intervals are narrow.\n\n\nAMBER\nThe methodology is robust but based on limited data; Data required significant transformation to fit the model; Confidence interval is quite wide; Data has not been reviewed recently.\n\n\nRED\nUnclear/unreliable data source or no data source provided; Based on limited data and methodology not robust; Data is not current; Confidence interval is wide or quality is unknown.\n\n\n\n\n\n\n\n\nAssumption ID\n\n\nDepends on Assumptions\n\n\nLocation in code, documentation or publication\n\n\nPlain English description of assumption\n\n\nBasis for assumption\n\n\nNumerical value of the assumption\n\n\nRange around the estimated value\n\n\nEstimated distribution\n\n\nLinks to supporting analysis\n\n\nDocumentation dependencies\n\n\nDate of last review/update\n\n\nExternally reviewed by\n\n\nDate of external review\n\n\nNext review/update due on\n\n\nQuality rating\n\n\nSensitivity score\n\n\nRisk score\n\n\n\n\n1\n\n\n2,3,4,5,6,7,8,9,10\n\n\nAssumption log\n\n\nWe assume that the dataset is representative of the population.\n\n\nTeam opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nDescriptive statistics (link), comparison to existing data source/publication (link)\n\n\nFinal report: methods, caveats\n\n\n14/02/2024\n\n\nJohn Doe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nHigh\n\n\nHigh\n\n\n\n\n2\n\n\n3,4,5,6,8,9,10\n\n\nAssumption log\n\n\nWe assume that the data does not exclude any population groups based on their demographic and socio-economic characteristics.\n\n\nTeam opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nDescriptive statistics (link), comparison to existing data source/publication (link)\n\n\nFinal report: methods, descriptive statistics, caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nHigh\n\n\nHigh\n\n\n\n\n3\n\n\n\n\nCorrespondence with data provider\n\n\nWe assume that all UK universities report data to the Higher Education Statistics Agency (HESA).\n\n\nValidated from data provider, coverage check against university list\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nHigh\n\n\nMedium\n\n\n\n\n4\n\n\n3\n\n\nCorrespondence with data provider\n\n\nWe assume that our list of UK universities is correct, current and comprehensive.\n\n\nTeam opinion, reliable source (HESA list)\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nGREEN\n\n\nLow\n\n\nMedium\n\n\n\n\n5\n\n\n2,3,4\n\n\nAssumption log\n\n\nWe assume that all universities accurately report the number of students enrolled during the academic year.\n\n\nExpert opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nFinal report: Caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nAMBER\n\n\nHigh\n\n\nHigh\n\n\n\n\n6\n\n\n3,4\n\n\nAssumption log\n\n\nWe assume that all universities accurately report the number of students who dropped out during the academic year.\n\n\nExpert opinion\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nPublication on drop-out rates\n\n\nFinal report: Caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n12/05/2024\n\n\nAMBER\n\n\nHigh\n\n\nHigh\n\n\n\n\n7\n\n\n3\n\n\nCorrespondence with data providers.\n\n\nWe assume that the academic year is consistently measured across UK universities.\n\n\nValidated from data provider\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nQuality assurance based on sampling universities from their websites\n\n\nQuality assurance log, final report: caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nRED\n\n\nHigh\n\n\nHigh\n\n\n\n\n8\n\n\n9\n\n\nCorrespondence with data providers.\n\n\nWe assume that students who receive special education services are excluded from the calculation of dropout rates.\n\n\nValidated from data provider\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nSensitivity analysis comparing dropout rates with and without this population included.\n\n\nQuality assurance log, final report: methods, caveats\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nRED\n\n\nHigh\n\n\nHigh\n\n\n\n\n9\n\n\n2,3,4,10\n\n\nExploratory data analysis notebook (link)\n\n\nWe assume that there is complete information for all the variables in the analysis.\n\n\nRobustness testing\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nDescriptive statistics notebook link\n\n\nDesk instructions, final report: methods, summarising the sample\n\n\n14/02/2024\n\n\nJane Roe\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nAMBER\n\n\nHigh\n\n\nHigh\n\n\n\n\n10\n\n\n2,3,4,9\n\n\nCorrespondence with data provider\n\n\nWe assume that the data collection process has not changed at all over time.\n\n\nValidated from data provider (link)\n\n\nN/A\n\n\nN/A\n\n\nN/A\n\n\nMethods documents from prior runs of this work, supplier data specifications and quality reports\n\n\nData supply specification, data quality report, methods document\n\n\nNot yet assigned\n\n\nNo external review\n\n\n14/02/2024\n\n\n14/05/2024\n\n\nRED\n\n\nHigh\n\n\nHigh\n\n\n\n\n11\n\n\n2,3,4,5,6,7,8\n\n\nAssumption log\n\n\nWe assume that the correlation coefficient between the dropout rate and social grade of local authority of origin is 0.7\n\n\nStatistical analysis of past data\n\n\n0.7\n\n\n+-0.1\n\n\nNormal\n\n\nCorrelation analysis report m(link), comparison to domain knowledge (link)\n\n\nData analysis documentation\n\n\n05/12/2023\n\n\nJohn Doe\n\n\n05/12/2023\n\n\n01/05/2024\n\n\nGREEN\n\n\nMedium\n\n\nLow"
  }
]